{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13bf5f60-1ad3-41f3-ae58-17185f68f4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15211</th>\n",
       "      <td>15212.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15212</th>\n",
       "      <td>15213.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15213</th>\n",
       "      <td>15214.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15214</th>\n",
       "      <td>15215.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15215</th>\n",
       "      <td>15216.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15216 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       instant     dteday  season   yr  mnth   hr  holiday  weekday  \\\n",
       "0          1.0   2011/1/1     1.0  0.0   1.0  0.0      0.0      6.0   \n",
       "1          2.0   2011/1/1     1.0  0.0   1.0  1.0      0.0      6.0   \n",
       "2          3.0   2011/1/1     1.0  0.0   1.0  2.0      0.0      6.0   \n",
       "3          4.0   2011/1/1     1.0  0.0   1.0  3.0      0.0      6.0   \n",
       "4          5.0   2011/1/1     1.0  0.0   1.0  4.0      0.0      6.0   \n",
       "...        ...        ...     ...  ...   ...  ...      ...      ...   \n",
       "15211  15212.0  2012/10/1     4.0  1.0  10.0  0.0      0.0      1.0   \n",
       "15212  15213.0  2012/10/1     4.0  1.0  10.0  1.0      0.0      1.0   \n",
       "15213  15214.0  2012/10/1     4.0  1.0  10.0  2.0      0.0      1.0   \n",
       "15214  15215.0  2012/10/1     4.0  1.0  10.0  3.0      0.0      1.0   \n",
       "15215  15216.0  2012/10/1     4.0  1.0  10.0  4.0      0.0      1.0   \n",
       "\n",
       "       workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
       "0             0.0         1.0  0.24  0.2879  0.81     0.0000     3.0   \n",
       "1             0.0         1.0  0.22  0.2727  0.80     0.0000     8.0   \n",
       "2             0.0         1.0  0.22  0.2727  0.80     0.0000     5.0   \n",
       "3             0.0         1.0  0.24  0.2879  0.75     0.0000     3.0   \n",
       "4             0.0         1.0  0.24  0.2879  0.75     0.0000     0.0   \n",
       "...           ...         ...   ...     ...   ...        ...     ...   \n",
       "15211         1.0         1.0  0.46  0.4545  0.72     0.1045     6.0   \n",
       "15212         1.0         1.0  0.44  0.4394  0.77     0.0896     5.0   \n",
       "15213         1.0         1.0  0.46  0.4545  0.72     0.0000     6.0   \n",
       "15214         1.0         1.0  0.44  0.4394  0.77     0.0000     1.0   \n",
       "15215         1.0         1.0  0.42  0.4242  0.82     0.1045     0.0   \n",
       "\n",
       "       registered   cnt  \n",
       "0            13.0  16.0  \n",
       "1            32.0  40.0  \n",
       "2            27.0  32.0  \n",
       "3            10.0  13.0  \n",
       "4             1.0   1.0  \n",
       "...           ...   ...  \n",
       "15211        39.0  45.0  \n",
       "15212        13.0  18.0  \n",
       "15213         6.0  12.0  \n",
       "15214         6.0   7.0  \n",
       "15215        10.0  10.0  \n",
       "\n",
       "[15216 rows x 17 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "#去空值\n",
    "train_data = train_data.iloc[:-3]\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ef9dcea-09c5-475d-ad08-e05ad0ee320b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>40</td>\n",
       "      <td>744</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>26</td>\n",
       "      <td>314</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5152</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>44</td>\n",
       "      <td>135</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>76</td>\n",
       "      <td>196</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>61</td>\n",
       "      <td>262</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155</th>\n",
       "      <td>2156</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>11</td>\n",
       "      <td>108</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>2157</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2158</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>2159</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>2160</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2160 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0           1   2012/10/1       4   1    10   8        0        1           1   \n",
       "1           2   2012/10/1       4   1    10   9        0        1           1   \n",
       "2           3   2012/10/1       4   1    10  10        0        1           1   \n",
       "3           4   2012/10/1       4   1    10  11        0        1           1   \n",
       "4           5   2012/10/1       4   1    10  12        0        1           1   \n",
       "...       ...         ...     ...  ..   ...  ..      ...      ...         ...   \n",
       "2155     2156  2012/12/31       1   1    12  19        0        1           1   \n",
       "2156     2157  2012/12/31       1   1    12  20        0        1           1   \n",
       "2157     2158  2012/12/31       1   1    12  21        0        1           1   \n",
       "2158     2159  2012/12/31       1   1    12  22        0        1           1   \n",
       "2159     2160  2012/12/31       1   1    12  23        0        1           1   \n",
       "\n",
       "      weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0              2  0.46  0.4545  0.77     0.1045      40         744  784  \n",
       "1              2  0.52  0.5000  0.63     0.0000      26         314  340  \n",
       "2              1  0.54  0.5152  0.56     0.0000      44         135  179  \n",
       "3              1  0.58  0.5455  0.46     0.0000      76         196  272  \n",
       "4              2  0.60  0.6212  0.43     0.1642      61         262  323  \n",
       "...          ...   ...     ...   ...        ...     ...         ...  ...  \n",
       "2155           2  0.26  0.2576  0.60     0.1642      11         108  119  \n",
       "2156           2  0.26  0.2576  0.60     0.1642       8          81   89  \n",
       "2157           1  0.26  0.2576  0.60     0.1642       7          83   90  \n",
       "2158           1  0.26  0.2727  0.56     0.1343      13          48   61  \n",
       "2159           1  0.26  0.2727  0.65     0.1343      12          37   49  \n",
       "\n",
       "[2160 rows x 17 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23f5116e-544f-44af-899d-746daff66c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated test_data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15121.0</td>\n",
       "      <td>2012/9/27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15122.0</td>\n",
       "      <td>2012/9/27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>5.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15123.0</td>\n",
       "      <td>2012/9/27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.5758</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15124.0</td>\n",
       "      <td>2012/9/27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.6061</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>20.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15125.0</td>\n",
       "      <td>2012/9/27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>335.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>2156.0</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>11.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>2157.0</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>8.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>2158.0</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>7.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>2159.0</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>13.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>2160.0</td>\n",
       "      <td>2012/12/31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>12.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2256 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      instant      dteday  season   yr  mnth    hr  holiday  weekday  \\\n",
       "0     15121.0   2012/9/27     4.0  1.0   9.0   5.0      0.0      4.0   \n",
       "1     15122.0   2012/9/27     4.0  1.0   9.0   6.0      0.0      4.0   \n",
       "2     15123.0   2012/9/27     4.0  1.0   9.0   7.0      0.0      4.0   \n",
       "3     15124.0   2012/9/27     4.0  1.0   9.0   8.0      0.0      4.0   \n",
       "4     15125.0   2012/9/27     4.0  1.0   9.0   9.0      0.0      4.0   \n",
       "...       ...         ...     ...  ...   ...   ...      ...      ...   \n",
       "2251   2156.0  2012/12/31     1.0  1.0  12.0  19.0      0.0      1.0   \n",
       "2252   2157.0  2012/12/31     1.0  1.0  12.0  20.0      0.0      1.0   \n",
       "2253   2158.0  2012/12/31     1.0  1.0  12.0  21.0      0.0      1.0   \n",
       "2254   2159.0  2012/12/31     1.0  1.0  12.0  22.0      0.0      1.0   \n",
       "2255   2160.0  2012/12/31     1.0  1.0  12.0  23.0      0.0      1.0   \n",
       "\n",
       "      workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
       "0            1.0         1.0  0.58  0.5455  0.83     0.0000     2.0   \n",
       "1            1.0         1.0  0.56  0.5303  0.88     0.0896     5.0   \n",
       "2            1.0         1.0  0.60  0.5758  0.78     0.0000    12.0   \n",
       "3            1.0         1.0  0.62  0.6061  0.71     0.0896    20.0   \n",
       "4            1.0         1.0  0.66  0.6212  0.65     0.0000    30.0   \n",
       "...          ...         ...   ...     ...   ...        ...     ...   \n",
       "2251         1.0         2.0  0.26  0.2576  0.60     0.1642    11.0   \n",
       "2252         1.0         2.0  0.26  0.2576  0.60     0.1642     8.0   \n",
       "2253         1.0         1.0  0.26  0.2576  0.60     0.1642     7.0   \n",
       "2254         1.0         1.0  0.26  0.2727  0.56     0.1343    13.0   \n",
       "2255         1.0         1.0  0.26  0.2727  0.65     0.1343    12.0   \n",
       "\n",
       "      registered    cnt  \n",
       "0           64.0   66.0  \n",
       "1          164.0  169.0  \n",
       "2          546.0  558.0  \n",
       "3          774.0  794.0  \n",
       "4          305.0  335.0  \n",
       "...          ...    ...  \n",
       "2251       108.0  119.0  \n",
       "2252        81.0   89.0  \n",
       "2253        83.0   90.0  \n",
       "2254        48.0   61.0  \n",
       "2255        37.0   49.0  \n",
       "\n",
       "[2256 rows x 17 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#提取train_data的最后96条数据\n",
    "last_96_rows = train_data.iloc[-96:]\n",
    "\n",
    "#将last_96_rows放到test_data的最前面\n",
    "test_data = pd.concat([last_96_rows, test_data], ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"\\nUpdated test_data:\")\n",
    "test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9eba5dff-6251-484e-b907-59623d44ede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_sin</th>\n",
       "      <th>season_cos</th>\n",
       "      <th>mnth_sin</th>\n",
       "      <th>mnth_cos</th>\n",
       "      <th>hr_sin</th>\n",
       "      <th>hr_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.629410</td>\n",
       "      <td>0.982963</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.982963</td>\n",
       "      <td>0.629410</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15211</th>\n",
       "      <td>15212.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.629410</td>\n",
       "      <td>0.982963</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15212</th>\n",
       "      <td>15213.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15213</th>\n",
       "      <td>15214.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15214</th>\n",
       "      <td>15215.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15215</th>\n",
       "      <td>15216.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.982963</td>\n",
       "      <td>0.629410</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15216 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       instant     dteday  season   yr  mnth   hr  holiday  weekday  \\\n",
       "0          1.0   2011/1/1     1.0  0.0   1.0  0.0      0.0      6.0   \n",
       "1          2.0   2011/1/1     1.0  0.0   1.0  1.0      0.0      6.0   \n",
       "2          3.0   2011/1/1     1.0  0.0   1.0  2.0      0.0      6.0   \n",
       "3          4.0   2011/1/1     1.0  0.0   1.0  3.0      0.0      6.0   \n",
       "4          5.0   2011/1/1     1.0  0.0   1.0  4.0      0.0      6.0   \n",
       "...        ...        ...     ...  ...   ...  ...      ...      ...   \n",
       "15211  15212.0  2012/10/1     4.0  1.0  10.0  0.0      0.0      1.0   \n",
       "15212  15213.0  2012/10/1     4.0  1.0  10.0  1.0      0.0      1.0   \n",
       "15213  15214.0  2012/10/1     4.0  1.0  10.0  2.0      0.0      1.0   \n",
       "15214  15215.0  2012/10/1     4.0  1.0  10.0  3.0      0.0      1.0   \n",
       "15215  15216.0  2012/10/1     4.0  1.0  10.0  4.0      0.0      1.0   \n",
       "\n",
       "       workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
       "0             0.0         1.0  0.24  0.2879  0.81     0.0000     3.0   \n",
       "1             0.0         1.0  0.22  0.2727  0.80     0.0000     8.0   \n",
       "2             0.0         1.0  0.22  0.2727  0.80     0.0000     5.0   \n",
       "3             0.0         1.0  0.24  0.2879  0.75     0.0000     3.0   \n",
       "4             0.0         1.0  0.24  0.2879  0.75     0.0000     0.0   \n",
       "...           ...         ...   ...     ...   ...        ...     ...   \n",
       "15211         1.0         1.0  0.46  0.4545  0.72     0.1045     6.0   \n",
       "15212         1.0         1.0  0.44  0.4394  0.77     0.0896     5.0   \n",
       "15213         1.0         1.0  0.46  0.4545  0.72     0.0000     6.0   \n",
       "15214         1.0         1.0  0.44  0.4394  0.77     0.0000     1.0   \n",
       "15215         1.0         1.0  0.42  0.4242  0.82     0.1045     0.0   \n",
       "\n",
       "       registered   cnt  season_sin  season_cos  mnth_sin  mnth_cos    hr_sin  \\\n",
       "0            13.0  16.0         1.0         0.5  0.750000  0.933013  0.629410   \n",
       "1            32.0  40.0         1.0         0.5  0.750000  0.933013  0.750000   \n",
       "2            27.0  32.0         1.0         0.5  0.750000  0.933013  0.853553   \n",
       "3            10.0  13.0         1.0         0.5  0.750000  0.933013  0.933013   \n",
       "4             1.0   1.0         1.0         0.5  0.750000  0.933013  0.982963   \n",
       "...           ...   ...         ...         ...       ...       ...       ...   \n",
       "15211        39.0  45.0         0.5         1.0  0.066987  0.750000  0.629410   \n",
       "15212        13.0  18.0         0.5         1.0  0.066987  0.750000  0.750000   \n",
       "15213         6.0  12.0         0.5         1.0  0.066987  0.750000  0.853553   \n",
       "15214         6.0   7.0         0.5         1.0  0.066987  0.750000  0.933013   \n",
       "15215        10.0  10.0         0.5         1.0  0.066987  0.750000  0.982963   \n",
       "\n",
       "         hr_cos  weekday_sin  weekday_cos  \n",
       "0      0.982963     0.500000      1.00000  \n",
       "1      0.933013     0.500000      1.00000  \n",
       "2      0.853553     0.500000      1.00000  \n",
       "3      0.750000     0.500000      1.00000  \n",
       "4      0.629410     0.500000      1.00000  \n",
       "...         ...          ...          ...  \n",
       "15211  0.982963     0.987464      0.38874  \n",
       "15212  0.933013     0.987464      0.38874  \n",
       "15213  0.853553     0.987464      0.38874  \n",
       "15214  0.750000     0.987464      0.38874  \n",
       "15215  0.629410     0.987464      0.38874  \n",
       "\n",
       "[15216 rows x 25 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cyclical_encode_auto_range(df, col_name):\n",
    "\n",
    "    #计算最小值和最大值\n",
    "    min_val = train_data[col_name].min()-1\n",
    "    max_val = train_data[col_name].max()\n",
    "    \n",
    "    #所有值都相同则返回\n",
    "    if min_val == max_val:\n",
    "        df[col_name + '_sin'] = 0.0\n",
    "        df[col_name + '_cos'] = 1.0\n",
    "        return df\n",
    "    \n",
    "    #将[min_val, max_val] 区间视为一个周期进行归一化\n",
    "    #将特征映射到[0, 1]区间\n",
    "    #然后乘以2*pi得到最终的相位\n",
    "    df[col_name + '_sin'] = (np.sin(\n",
    "        2 * np.pi * (df[col_name] - min_val) / (max_val - min_val)\n",
    "    ) + 1) / 2\n",
    "    df[col_name + '_cos'] = (np.cos(\n",
    "        2 * np.pi * (df[col_name] - min_val) / (max_val - min_val)\n",
    "    ) + 1) / 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_data = cyclical_encode_auto_range(train_data, 'season')\n",
    "train_data = cyclical_encode_auto_range(train_data, 'mnth')\n",
    "train_data = cyclical_encode_auto_range(train_data, 'hr')\n",
    "train_data = cyclical_encode_auto_range(train_data, 'weekday')\n",
    "test_data = cyclical_encode_auto_range(test_data, 'season')\n",
    "test_data = cyclical_encode_auto_range(test_data, 'mnth')\n",
    "test_data = cyclical_encode_auto_range(test_data, 'hr')\n",
    "test_data = cyclical_encode_auto_range(test_data, 'weekday')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "683683de-4970-460e-a957-95ceb7723897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc26f847-6d02-4503-916a-314caf5f37bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_sin</th>\n",
       "      <th>season_cos</th>\n",
       "      <th>mnth_sin</th>\n",
       "      <th>mnth_cos</th>\n",
       "      <th>hr_sin</th>\n",
       "      <th>hr_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.015369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.629410</td>\n",
       "      <td>0.982963</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.021798</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>0.039959</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008174</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2011/1/1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.982963</td>\n",
       "      <td>0.629410</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15211</th>\n",
       "      <td>15212.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0.016349</td>\n",
       "      <td>0.044018</td>\n",
       "      <td>0.045082</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.629410</td>\n",
       "      <td>0.982963</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15212</th>\n",
       "      <td>15213.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.013624</td>\n",
       "      <td>0.014673</td>\n",
       "      <td>0.017418</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15213</th>\n",
       "      <td>15214.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.016349</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.853553</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15214</th>\n",
       "      <td>15215.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.4394</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15215</th>\n",
       "      <td>15216.0</td>\n",
       "      <td>2012/10/1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.982963</td>\n",
       "      <td>0.629410</td>\n",
       "      <td>0.987464</td>\n",
       "      <td>0.38874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15216 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       instant     dteday  season   yr  mnth   hr  holiday  weekday  \\\n",
       "0          1.0   2011/1/1     1.0  0.0   1.0  0.0      0.0      6.0   \n",
       "1          2.0   2011/1/1     1.0  0.0   1.0  1.0      0.0      6.0   \n",
       "2          3.0   2011/1/1     1.0  0.0   1.0  2.0      0.0      6.0   \n",
       "3          4.0   2011/1/1     1.0  0.0   1.0  3.0      0.0      6.0   \n",
       "4          5.0   2011/1/1     1.0  0.0   1.0  4.0      0.0      6.0   \n",
       "...        ...        ...     ...  ...   ...  ...      ...      ...   \n",
       "15211  15212.0  2012/10/1     4.0  1.0  10.0  0.0      0.0      1.0   \n",
       "15212  15213.0  2012/10/1     4.0  1.0  10.0  1.0      0.0      1.0   \n",
       "15213  15214.0  2012/10/1     4.0  1.0  10.0  2.0      0.0      1.0   \n",
       "15214  15215.0  2012/10/1     4.0  1.0  10.0  3.0      0.0      1.0   \n",
       "15215  15216.0  2012/10/1     4.0  1.0  10.0  4.0      0.0      1.0   \n",
       "\n",
       "       workingday  weathersit  temp   atemp   hum  windspeed    casual  \\\n",
       "0             0.0         0.0  0.24  0.2879  0.81     0.0000  0.008174   \n",
       "1             0.0         0.0  0.22  0.2727  0.80     0.0000  0.021798   \n",
       "2             0.0         0.0  0.22  0.2727  0.80     0.0000  0.013624   \n",
       "3             0.0         0.0  0.24  0.2879  0.75     0.0000  0.008174   \n",
       "4             0.0         0.0  0.24  0.2879  0.75     0.0000  0.000000   \n",
       "...           ...         ...   ...     ...   ...        ...       ...   \n",
       "15211         1.0         0.0  0.46  0.4545  0.72     0.1045  0.016349   \n",
       "15212         1.0         0.0  0.44  0.4394  0.77     0.0896  0.013624   \n",
       "15213         1.0         0.0  0.46  0.4545  0.72     0.0000  0.016349   \n",
       "15214         1.0         0.0  0.44  0.4394  0.77     0.0000  0.002725   \n",
       "15215         1.0         0.0  0.42  0.4242  0.82     0.1045  0.000000   \n",
       "\n",
       "       registered       cnt  season_sin  season_cos  mnth_sin  mnth_cos  \\\n",
       "0        0.014673  0.015369         1.0         0.5  0.750000  0.933013   \n",
       "1        0.036117  0.039959         1.0         0.5  0.750000  0.933013   \n",
       "2        0.030474  0.031762         1.0         0.5  0.750000  0.933013   \n",
       "3        0.011287  0.012295         1.0         0.5  0.750000  0.933013   \n",
       "4        0.001129  0.000000         1.0         0.5  0.750000  0.933013   \n",
       "...           ...       ...         ...         ...       ...       ...   \n",
       "15211    0.044018  0.045082         0.5         1.0  0.066987  0.750000   \n",
       "15212    0.014673  0.017418         0.5         1.0  0.066987  0.750000   \n",
       "15213    0.006772  0.011270         0.5         1.0  0.066987  0.750000   \n",
       "15214    0.006772  0.006148         0.5         1.0  0.066987  0.750000   \n",
       "15215    0.011287  0.009221         0.5         1.0  0.066987  0.750000   \n",
       "\n",
       "         hr_sin    hr_cos  weekday_sin  weekday_cos  \n",
       "0      0.629410  0.982963     0.500000      1.00000  \n",
       "1      0.750000  0.933013     0.500000      1.00000  \n",
       "2      0.853553  0.853553     0.500000      1.00000  \n",
       "3      0.933013  0.750000     0.500000      1.00000  \n",
       "4      0.982963  0.629410     0.500000      1.00000  \n",
       "...         ...       ...          ...          ...  \n",
       "15211  0.629410  0.982963     0.987464      0.38874  \n",
       "15212  0.750000  0.933013     0.987464      0.38874  \n",
       "15213  0.853553  0.853553     0.987464      0.38874  \n",
       "15214  0.933013  0.750000     0.987464      0.38874  \n",
       "15215  0.982963  0.629410     0.987464      0.38874  \n",
       "\n",
       "[15216 rows x 25 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize_column(df, col_name):\n",
    "    \n",
    "    #计算最小值和最大值\n",
    "    min_val = df[col_name].min()\n",
    "    max_val = df[col_name].max()\n",
    "\n",
    "    #防止最小值和最大值相等\n",
    "    if min_val == max_val:\n",
    "        df[col_name] = 0.0\n",
    "    else:\n",
    "        #进行归一化\n",
    "        df[col_name] = (df[col_name] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_data = normalize_column(train_data, 'weathersit')\n",
    "train_data = normalize_column(train_data, 'casual')\n",
    "train_data = normalize_column(train_data, 'registered')\n",
    "train_data = normalize_column(train_data, 'cnt')\n",
    "test_data = normalize_column(test_data, 'weathersit')\n",
    "test_data = normalize_column(test_data, 'casual')\n",
    "test_data = normalize_column(test_data, 'registered')\n",
    "test_data = normalize_column(test_data, 'cnt')\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cba5f041-92bd-4417-9f4d-b1b2470da3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dteday 列是依次递增的。\n"
     ]
    }
   ],
   "source": [
    "train_data['dteday'] = pd.to_datetime(train_data['dteday'])\n",
    "#检查dteday列是否递增\n",
    "is_increasing = train_data['dteday'].is_monotonic_increasing\n",
    "\n",
    "#输出结果\n",
    "if is_increasing:\n",
    "    print(\"dteday 列是依次递增的。\")\n",
    "else:\n",
    "    print(\"dteday 列不是依次递增的。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d99dfcc6-ce96-4caa-9fe8-01352df4a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data 的列数是：17\n",
      "train_data每列的最小值：\n",
      "yr             0.000000\n",
      "holiday        0.000000\n",
      "workingday     0.000000\n",
      "weathersit     0.000000\n",
      "temp           0.020000\n",
      "atemp          0.000000\n",
      "hum            0.000000\n",
      "windspeed      0.000000\n",
      "cnt            0.000000\n",
      "season_sin     0.000000\n",
      "season_cos     0.000000\n",
      "mnth_sin       0.000000\n",
      "mnth_cos       0.000000\n",
      "hr_sin         0.000000\n",
      "hr_cos         0.000000\n",
      "weekday_sin    0.012536\n",
      "weekday_cos    0.049516\n",
      "dtype: float64\n",
      "\n",
      "train_data每列的最大值：\n",
      "yr             1.000000\n",
      "holiday        1.000000\n",
      "workingday     1.000000\n",
      "weathersit     1.000000\n",
      "temp           1.000000\n",
      "atemp          1.000000\n",
      "hum            1.000000\n",
      "windspeed      0.850700\n",
      "cnt            1.000000\n",
      "season_sin     1.000000\n",
      "season_cos     1.000000\n",
      "mnth_sin       1.000000\n",
      "mnth_cos       1.000000\n",
      "hr_sin         1.000000\n",
      "hr_cos         1.000000\n",
      "weekday_sin    0.987464\n",
      "weekday_cos    1.000000\n",
      "dtype: float64\n",
      "test_data每列的最小值：\n",
      "yr             1.000000\n",
      "holiday        0.000000\n",
      "workingday     0.000000\n",
      "weathersit     0.000000\n",
      "temp           0.140000\n",
      "atemp          0.151500\n",
      "hum            0.160000\n",
      "windspeed      0.000000\n",
      "cnt            0.000000\n",
      "season_sin     0.500000\n",
      "season_cos     0.500000\n",
      "mnth_sin       0.000000\n",
      "mnth_cos       0.500000\n",
      "hr_sin         0.000000\n",
      "hr_cos         0.000000\n",
      "weekday_sin    0.012536\n",
      "weekday_cos    0.049516\n",
      "dtype: float64\n",
      "\n",
      "test_data每列的最大值：\n",
      "yr             1.000000\n",
      "holiday        1.000000\n",
      "workingday     1.000000\n",
      "weathersit     1.000000\n",
      "temp           0.760000\n",
      "atemp          0.681800\n",
      "hum            1.000000\n",
      "windspeed      0.656700\n",
      "cnt            1.000000\n",
      "season_sin     1.000000\n",
      "season_cos     1.000000\n",
      "mnth_sin       0.500000\n",
      "mnth_cos       1.000000\n",
      "hr_sin         1.000000\n",
      "hr_cos         1.000000\n",
      "weekday_sin    0.987464\n",
      "weekday_cos    1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.drop(columns=['instant', 'dteday', 'season', 'mnth', 'hr', 'weekday', 'casual', 'registered'])\n",
    "test_data = test_data.drop(columns=['instant', 'dteday', 'season', 'mnth', 'hr', 'weekday', 'casual', 'registered'])\n",
    "\n",
    "num_columns = train_data.shape[1]\n",
    "\n",
    "#输出列数\n",
    "print(f\"train_data 的列数是：{num_columns}\")\n",
    "\n",
    "#查看每列的最小值\n",
    "print(\"train_data每列的最小值：\")\n",
    "print(train_data.min())\n",
    "\n",
    "#查看每列的最大值\n",
    "print(\"\\ntrain_data每列的最大值：\")\n",
    "print(train_data.max())\n",
    "\n",
    "#查看每列的最小值\n",
    "print(\"test_data每列的最小值：\")\n",
    "print(test_data.min())\n",
    "\n",
    "#查看每列的最大值\n",
    "print(\"\\ntest_data每列的最大值：\")\n",
    "print(test_data.max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83a1b514-b4b1-4128-8854-d4ebba218d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[[c for c in train_data.columns if c != 'cnt'] + ['cnt']]\n",
    "test_data = test_data[[c for c in test_data.columns if c != 'cnt'] + ['cnt']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "417a1600-b796-4c3b-b630-6655948e7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#此时所有数据都归一化完成，现在进行滚动预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a3e1335-32f1-4cb3-bcd6-69b9b6da6fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def create_dataset(data_array, n_past=96, n_future=96):\n",
    "    \"\"\"\n",
    "    data_array: shape (total_length, 17)\n",
    "    n_past: int, 过去时间步数\n",
    "    n_future: int, 需要预测的未来时间步数\n",
    "    返回:\n",
    "      X_enc: (样本数, n_past, 17)\n",
    "      X_dec: (样本数, n_future, 16)\n",
    "      Y:     (样本数, n_future, 1)\n",
    "    \"\"\"\n",
    "    X_enc_list = []\n",
    "    X_dec_list = []\n",
    "    Y_list = []\n",
    "    \n",
    "    total_length = len(data_array)\n",
    "    \n",
    "    for i in range(total_length - n_past - n_future):\n",
    "        #encoder输入\n",
    "        encoder_input = data_array[i : i + n_past]  \n",
    "\n",
    "        #decoder输入\n",
    "        decoder_input = data_array[i + n_past : i + n_past + n_future, :16]  \n",
    "\n",
    "        #目标值\n",
    "        label = data_array[i + n_past : i + n_past + n_future, 16:17]  \n",
    "        \n",
    "        X_enc_list.append(encoder_input)\n",
    "        X_dec_list.append(decoder_input)\n",
    "        Y_list.append(label)\n",
    "    \n",
    "    X_enc = np.array(X_enc_list)  \n",
    "    X_dec = np.array(X_dec_list)  \n",
    "    Y = np.array(Y_list)          \n",
    "    \n",
    "    return X_enc, X_dec, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bacfe39b-1e48-4157-953e-f1bc608704cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, seq_length: int = 96):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.X_enc, self.X_dec, self.Y = create_dataset(data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_enc)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_enc = self.X_enc[idx] \n",
    "        x_dec = self.X_dec[idx]  \n",
    "        y = self.Y[idx]          \n",
    "        \n",
    "        #转为torch.Tensor\n",
    "        x_enc = torch.FloatTensor(x_enc)\n",
    "        x_dec = torch.FloatTensor(x_dec)\n",
    "        y = torch.FloatTensor(y)\n",
    "        \n",
    "        return x_enc, x_dec, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4886d881-30a5-4ff6-898b-58bcd95b95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTimeSeriesModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model=64, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=2, \n",
    "        num_decoder_layers=2, \n",
    "        dim_feedforward=256, \n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #用线性层将输入维度映射到d_model\n",
    "        self.encoder_input_layer = nn.Linear(17, d_model)\n",
    "        self.decoder_input_layer = nn.Linear(16, d_model)\n",
    "        \n",
    "        #定义Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False  \n",
    "        )\n",
    "        \n",
    "        #最终输出层\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "       \n",
    "        #先投射到d_model\n",
    "        src_emb = self.encoder_input_layer(src)  \n",
    "        tgt_emb = self.decoder_input_layer(tgt)  \n",
    "        \n",
    "        \n",
    "        src_emb = src_emb.permute(1, 0, 2)  \n",
    "        tgt_emb = tgt_emb.permute(1, 0, 2)  \n",
    "        \n",
    "        #Transformer输出\n",
    "        transformer_out = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            src_mask=None,\n",
    "            tgt_mask=None,\n",
    "            memory_mask=None\n",
    "        )\n",
    "        \n",
    "        \n",
    "        transformer_out = transformer_out.permute(1, 0, 2)\n",
    "        \n",
    "        #最后映射到1维输出\n",
    "        out = self.output_layer(transformer_out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8a9ef5d-66d2-47c0-92a5-4d68db7d25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_mse = 0.0\n",
    "    running_mae = 0.0\n",
    "    \n",
    "    for x_enc, x_dec, y in dataloader:\n",
    "        x_enc = x_enc.to(device)\n",
    "        x_dec = x_dec.to(device)\n",
    "        y = y.to(device).squeeze(-1)\n",
    "        \n",
    "        outputs = model(x_enc, x_dec).squeeze(-1)\n",
    "        mse_loss = criterion(outputs, y)\n",
    "        mae_loss = torch.mean(torch.abs(outputs - y))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #反向传播\n",
    "        mse_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += mse_loss.item() * x_enc.size(0)\n",
    "        running_mse += mse_loss.item() * x_enc.size(0)\n",
    "        running_mae += mae_loss.item() * x_enc.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_mse = running_mse / len(dataloader.dataset)\n",
    "    epoch_mae = running_mae / len(dataloader.dataset)\n",
    "    return epoch_loss, epoch_mse, epoch_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f603f6c9-1ca0-4c5a-8a95-b565a4f4df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_mse = 0.0\n",
    "    running_mae = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y in dataloader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y = y.to(device).squeeze(-1)\n",
    "\n",
    "            outputs = model(x_enc, x_dec).squeeze(-1)\n",
    "            mse_loss = criterion(outputs, y)\n",
    "            mae_loss = torch.mean(torch.abs(outputs - y))\n",
    "            \n",
    "            running_mse += mse_loss.item() * x_enc.size(0)\n",
    "            running_mae += mae_loss.item() * x_enc.size(0)\n",
    "    \n",
    "    epoch_mse = running_mse / len(dataloader.dataset)\n",
    "    epoch_mae = running_mae / len(dataloader.dataset)\n",
    "    return epoch_mse, epoch_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d379d697-10d4-4322-9d9b-ba0b04b14131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#超参数\n",
    "SEQ_LENGTH = 96\n",
    "INPUT_SIZE = 17\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd16a62b-ab48-41b1-bd68-8bbf54fe11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将DataFrame转为数组\n",
    "train_array = train_data.values  \n",
    "test_array = test_data.values    \n",
    "\n",
    "#构建Dataset\n",
    "train_dataset = TimeSeriesDataset(train_array, seq_length=SEQ_LENGTH)\n",
    "test_dataset = TimeSeriesDataset(test_array, seq_length=SEQ_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c37a8d8-2b80-4080-a4ce-c52ccb03635c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/zczhang/anaconda3/envs/py38lyd/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.062878 | Train MSE: 0.062878 | Train MAE: 0.137603 Val MSE: 0.017292 | Val MAE: 0.087872\n",
      "Epoch [2/100] Train Loss: 0.003814 | Train MSE: 0.003814 | Train MAE: 0.045061 Val MSE: 0.012154 | Val MAE: 0.072154\n",
      "Epoch [3/100] Train Loss: 0.002632 | Train MSE: 0.002632 | Train MAE: 0.037138 Val MSE: 0.012512 | Val MAE: 0.073686\n",
      "Epoch [4/100] Train Loss: 0.002095 | Train MSE: 0.002095 | Train MAE: 0.032951 Val MSE: 0.009533 | Val MAE: 0.063213\n",
      "Epoch [5/100] Train Loss: 0.001779 | Train MSE: 0.001779 | Train MAE: 0.030337 Val MSE: 0.008126 | Val MAE: 0.057850\n",
      "Epoch [6/100] Train Loss: 0.001563 | Train MSE: 0.001563 | Train MAE: 0.028465 Val MSE: 0.006238 | Val MAE: 0.049670\n",
      "Epoch [7/100] Train Loss: 0.001395 | Train MSE: 0.001395 | Train MAE: 0.026959 Val MSE: 0.007464 | Val MAE: 0.055503\n",
      "Epoch [8/100] Train Loss: 0.001248 | Train MSE: 0.001248 | Train MAE: 0.025504 Val MSE: 0.006068 | Val MAE: 0.049759\n",
      "Epoch [9/100] Train Loss: 0.001142 | Train MSE: 0.001142 | Train MAE: 0.024397 Val MSE: 0.006837 | Val MAE: 0.052742\n",
      "Epoch [10/100] Train Loss: 0.001045 | Train MSE: 0.001045 | Train MAE: 0.023389 Val MSE: 0.005773 | Val MAE: 0.047574\n",
      "Epoch [11/100] Train Loss: 0.000956 | Train MSE: 0.000956 | Train MAE: 0.022408 Val MSE: 0.005197 | Val MAE: 0.044691\n",
      "Epoch [12/100] Train Loss: 0.000962 | Train MSE: 0.000962 | Train MAE: 0.022253 Val MSE: 0.007651 | Val MAE: 0.058381\n",
      "Epoch [13/100] Train Loss: 0.000822 | Train MSE: 0.000822 | Train MAE: 0.020876 Val MSE: 0.005704 | Val MAE: 0.047488\n",
      "Epoch [14/100] Train Loss: 0.000759 | Train MSE: 0.000759 | Train MAE: 0.020135 Val MSE: 0.005658 | Val MAE: 0.047462\n",
      "Epoch [15/100] Train Loss: 0.000716 | Train MSE: 0.000716 | Train MAE: 0.019593 Val MSE: 0.005257 | Val MAE: 0.044941\n",
      "Epoch [16/100] Train Loss: 0.000679 | Train MSE: 0.000679 | Train MAE: 0.019104 Val MSE: 0.005087 | Val MAE: 0.044669\n",
      "Epoch [17/100] Train Loss: 0.000631 | Train MSE: 0.000631 | Train MAE: 0.018381 Val MSE: 0.005371 | Val MAE: 0.045433\n",
      "Epoch [18/100] Train Loss: 0.000594 | Train MSE: 0.000594 | Train MAE: 0.017910 Val MSE: 0.004841 | Val MAE: 0.042674\n",
      "Epoch [19/100] Train Loss: 0.000556 | Train MSE: 0.000556 | Train MAE: 0.017339 Val MSE: 0.004765 | Val MAE: 0.042101\n",
      "Epoch [20/100] Train Loss: 0.000528 | Train MSE: 0.000528 | Train MAE: 0.016896 Val MSE: 0.004278 | Val MAE: 0.039656\n",
      "Epoch [21/100] Train Loss: 0.000525 | Train MSE: 0.000525 | Train MAE: 0.016731 Val MSE: 0.004365 | Val MAE: 0.039555\n",
      "Epoch [22/100] Train Loss: 0.000477 | Train MSE: 0.000477 | Train MAE: 0.016031 Val MSE: 0.004664 | Val MAE: 0.040992\n",
      "Epoch [23/100] Train Loss: 0.000463 | Train MSE: 0.000463 | Train MAE: 0.015822 Val MSE: 0.004994 | Val MAE: 0.042115\n",
      "Epoch [24/100] Train Loss: 0.000627 | Train MSE: 0.000627 | Train MAE: 0.017458 Val MSE: 0.004560 | Val MAE: 0.042134\n",
      "Epoch [25/100] Train Loss: 0.000401 | Train MSE: 0.000401 | Train MAE: 0.014761 Val MSE: 0.005015 | Val MAE: 0.045241\n",
      "Epoch [26/100] Train Loss: 0.000383 | Train MSE: 0.000383 | Train MAE: 0.014457 Val MSE: 0.004986 | Val MAE: 0.043953\n",
      "Epoch [27/100] Train Loss: 0.000372 | Train MSE: 0.000372 | Train MAE: 0.014178 Val MSE: 0.005297 | Val MAE: 0.044054\n",
      "Epoch [28/100] Train Loss: 0.000346 | Train MSE: 0.000346 | Train MAE: 0.013710 Val MSE: 0.004918 | Val MAE: 0.043913\n",
      "Epoch [29/100] Train Loss: 0.000355 | Train MSE: 0.000355 | Train MAE: 0.013805 Val MSE: 0.004975 | Val MAE: 0.045810\n",
      "Epoch [30/100] Train Loss: 0.000339 | Train MSE: 0.000339 | Train MAE: 0.013473 Val MSE: 0.004176 | Val MAE: 0.039612\n",
      "Epoch [31/100] Train Loss: 0.000312 | Train MSE: 0.000312 | Train MAE: 0.012951 Val MSE: 0.004612 | Val MAE: 0.043243\n",
      "Epoch [32/100] Train Loss: 0.000273 | Train MSE: 0.000273 | Train MAE: 0.012217 Val MSE: 0.004769 | Val MAE: 0.043375\n",
      "Epoch [33/100] Train Loss: 0.000288 | Train MSE: 0.000288 | Train MAE: 0.012461 Val MSE: 0.003827 | Val MAE: 0.039649\n",
      "Epoch [34/100] Train Loss: 0.000302 | Train MSE: 0.000302 | Train MAE: 0.012614 Val MSE: 0.005024 | Val MAE: 0.045359\n",
      "Epoch [35/100] Train Loss: 0.000235 | Train MSE: 0.000235 | Train MAE: 0.011313 Val MSE: 0.004219 | Val MAE: 0.041258\n",
      "Epoch [36/100] Train Loss: 0.000218 | Train MSE: 0.000218 | Train MAE: 0.010975 Val MSE: 0.005376 | Val MAE: 0.047649\n",
      "Epoch [37/100] Train Loss: 0.000214 | Train MSE: 0.000214 | Train MAE: 0.010792 Val MSE: 0.004932 | Val MAE: 0.044640\n",
      "Epoch [38/100] Train Loss: 0.000342 | Train MSE: 0.000342 | Train MAE: 0.012606 Val MSE: 0.005069 | Val MAE: 0.045388\n",
      "Epoch [39/100] Train Loss: 0.000227 | Train MSE: 0.000227 | Train MAE: 0.011154 Val MSE: 0.005019 | Val MAE: 0.045676\n",
      "Epoch [40/100] Train Loss: 0.000194 | Train MSE: 0.000194 | Train MAE: 0.010292 Val MSE: 0.005795 | Val MAE: 0.050693\n",
      "Epoch [41/100] Train Loss: 0.000182 | Train MSE: 0.000182 | Train MAE: 0.009956 Val MSE: 0.005942 | Val MAE: 0.050084\n",
      "Epoch [42/100] Train Loss: 0.000171 | Train MSE: 0.000171 | Train MAE: 0.009736 Val MSE: 0.005672 | Val MAE: 0.049619\n",
      "Epoch [43/100] Train Loss: 0.000179 | Train MSE: 0.000179 | Train MAE: 0.009897 Val MSE: 0.005346 | Val MAE: 0.048065\n",
      "Epoch [44/100] Train Loss: 0.000196 | Train MSE: 0.000196 | Train MAE: 0.010290 Val MSE: 0.005915 | Val MAE: 0.051103\n",
      "Epoch [45/100] Train Loss: 0.000196 | Train MSE: 0.000196 | Train MAE: 0.010204 Val MSE: 0.006185 | Val MAE: 0.052919\n",
      "Epoch [46/100] Train Loss: 0.000155 | Train MSE: 0.000155 | Train MAE: 0.009233 Val MSE: 0.005400 | Val MAE: 0.049033\n",
      "Epoch [47/100] Train Loss: 0.000306 | Train MSE: 0.000306 | Train MAE: 0.011845 Val MSE: 0.005561 | Val MAE: 0.050994\n",
      "Epoch [48/100] Train Loss: 0.000144 | Train MSE: 0.000144 | Train MAE: 0.008906 Val MSE: 0.005822 | Val MAE: 0.052350\n",
      "Epoch [49/100] Train Loss: 0.000137 | Train MSE: 0.000137 | Train MAE: 0.008655 Val MSE: 0.005763 | Val MAE: 0.051125\n",
      "Epoch [50/100] Train Loss: 0.000139 | Train MSE: 0.000139 | Train MAE: 0.008753 Val MSE: 0.006852 | Val MAE: 0.056364\n",
      "Epoch [51/100] Train Loss: 0.000131 | Train MSE: 0.000131 | Train MAE: 0.008521 Val MSE: 0.005316 | Val MAE: 0.048321\n",
      "Epoch [52/100] Train Loss: 0.000158 | Train MSE: 0.000158 | Train MAE: 0.009098 Val MSE: 0.005119 | Val MAE: 0.047554\n",
      "Epoch [53/100] Train Loss: 0.000140 | Train MSE: 0.000140 | Train MAE: 0.008689 Val MSE: 0.005899 | Val MAE: 0.050948\n",
      "Epoch [54/100] Train Loss: 0.000216 | Train MSE: 0.000216 | Train MAE: 0.010350 Val MSE: 0.005584 | Val MAE: 0.049036\n",
      "Epoch [55/100] Train Loss: 0.000130 | Train MSE: 0.000130 | Train MAE: 0.008414 Val MSE: 0.005764 | Val MAE: 0.050050\n",
      "Epoch [56/100] Train Loss: 0.000119 | Train MSE: 0.000119 | Train MAE: 0.008122 Val MSE: 0.005705 | Val MAE: 0.050379\n",
      "Epoch [57/100] Train Loss: 0.000110 | Train MSE: 0.000110 | Train MAE: 0.007822 Val MSE: 0.005520 | Val MAE: 0.049625\n",
      "Epoch [58/100] Train Loss: 0.000112 | Train MSE: 0.000112 | Train MAE: 0.007875 Val MSE: 0.005443 | Val MAE: 0.049920\n",
      "Epoch [59/100] Train Loss: 0.000146 | Train MSE: 0.000146 | Train MAE: 0.008803 Val MSE: 0.006211 | Val MAE: 0.052077\n",
      "Epoch [60/100] Train Loss: 0.000231 | Train MSE: 0.000231 | Train MAE: 0.010361 Val MSE: 0.005775 | Val MAE: 0.050575\n",
      "Epoch [61/100] Train Loss: 0.000112 | Train MSE: 0.000112 | Train MAE: 0.007842 Val MSE: 0.005754 | Val MAE: 0.050057\n",
      "Epoch [62/100] Train Loss: 0.000101 | Train MSE: 0.000101 | Train MAE: 0.007494 Val MSE: 0.005899 | Val MAE: 0.051571\n",
      "Epoch [63/100] Train Loss: 0.000107 | Train MSE: 0.000107 | Train MAE: 0.007672 Val MSE: 0.005894 | Val MAE: 0.051149\n",
      "Epoch [64/100] Train Loss: 0.000154 | Train MSE: 0.000154 | Train MAE: 0.008720 Val MSE: 0.005963 | Val MAE: 0.052247\n",
      "Epoch [65/100] Train Loss: 0.000140 | Train MSE: 0.000140 | Train MAE: 0.008542 Val MSE: 0.006117 | Val MAE: 0.052954\n",
      "Epoch [66/100] Train Loss: 0.000132 | Train MSE: 0.000132 | Train MAE: 0.008287 Val MSE: 0.006008 | Val MAE: 0.051470\n",
      "Epoch [67/100] Train Loss: 0.000098 | Train MSE: 0.000098 | Train MAE: 0.007354 Val MSE: 0.005834 | Val MAE: 0.051182\n",
      "Epoch [68/100] Train Loss: 0.000091 | Train MSE: 0.000091 | Train MAE: 0.007155 Val MSE: 0.005922 | Val MAE: 0.051847\n",
      "Epoch [69/100] Train Loss: 0.000091 | Train MSE: 0.000091 | Train MAE: 0.007115 Val MSE: 0.006512 | Val MAE: 0.054437\n",
      "Epoch [70/100] Train Loss: 0.000183 | Train MSE: 0.000183 | Train MAE: 0.009311 Val MSE: 0.006011 | Val MAE: 0.052280\n",
      "Epoch [71/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.007037 Val MSE: 0.005907 | Val MAE: 0.051771\n",
      "Epoch [72/100] Train Loss: 0.000085 | Train MSE: 0.000085 | Train MAE: 0.006903 Val MSE: 0.005623 | Val MAE: 0.049594\n",
      "Epoch [73/100] Train Loss: 0.000097 | Train MSE: 0.000097 | Train MAE: 0.007311 Val MSE: 0.006238 | Val MAE: 0.051563\n",
      "Epoch [74/100] Train Loss: 0.000101 | Train MSE: 0.000101 | Train MAE: 0.007412 Val MSE: 0.005887 | Val MAE: 0.051148\n",
      "Epoch [75/100] Train Loss: 0.000130 | Train MSE: 0.000130 | Train MAE: 0.007713 Val MSE: 0.008944 | Val MAE: 0.062940\n",
      "Epoch [76/100] Train Loss: 0.000317 | Train MSE: 0.000317 | Train MAE: 0.011191 Val MSE: 0.006356 | Val MAE: 0.053834\n",
      "Epoch [77/100] Train Loss: 0.000106 | Train MSE: 0.000106 | Train MAE: 0.007524 Val MSE: 0.006369 | Val MAE: 0.053257\n",
      "Epoch [78/100] Train Loss: 0.000093 | Train MSE: 0.000093 | Train MAE: 0.007098 Val MSE: 0.006108 | Val MAE: 0.052443\n",
      "Epoch [79/100] Train Loss: 0.000091 | Train MSE: 0.000091 | Train MAE: 0.007017 Val MSE: 0.006260 | Val MAE: 0.052950\n",
      "Epoch [80/100] Train Loss: 0.000095 | Train MSE: 0.000095 | Train MAE: 0.007152 Val MSE: 0.006043 | Val MAE: 0.051907\n",
      "Epoch [81/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.006991 Val MSE: 0.005662 | Val MAE: 0.050092\n",
      "Epoch [82/100] Train Loss: 0.000082 | Train MSE: 0.000082 | Train MAE: 0.006774 Val MSE: 0.005602 | Val MAE: 0.050498\n",
      "Epoch [83/100] Train Loss: 0.000092 | Train MSE: 0.000092 | Train MAE: 0.007051 Val MSE: 0.005708 | Val MAE: 0.050359\n",
      "Epoch [84/100] Train Loss: 0.000115 | Train MSE: 0.000115 | Train MAE: 0.007716 Val MSE: 0.005993 | Val MAE: 0.051767\n",
      "Epoch [85/100] Train Loss: 0.000094 | Train MSE: 0.000094 | Train MAE: 0.007142 Val MSE: 0.006000 | Val MAE: 0.051814\n",
      "Epoch [86/100] Train Loss: 0.000096 | Train MSE: 0.000096 | Train MAE: 0.007152 Val MSE: 0.006167 | Val MAE: 0.052660\n",
      "Epoch [87/100] Train Loss: 0.000079 | Train MSE: 0.000079 | Train MAE: 0.006594 Val MSE: 0.005736 | Val MAE: 0.050911\n",
      "Epoch [88/100] Train Loss: 0.000190 | Train MSE: 0.000190 | Train MAE: 0.009432 Val MSE: 0.005829 | Val MAE: 0.050673\n",
      "Epoch [89/100] Train Loss: 0.000097 | Train MSE: 0.000097 | Train MAE: 0.007196 Val MSE: 0.006039 | Val MAE: 0.052203\n",
      "Epoch [90/100] Train Loss: 0.000077 | Train MSE: 0.000077 | Train MAE: 0.006521 Val MSE: 0.006101 | Val MAE: 0.052794\n",
      "Epoch [91/100] Train Loss: 0.000075 | Train MSE: 0.000075 | Train MAE: 0.006431 Val MSE: 0.005943 | Val MAE: 0.051848\n",
      "Epoch [92/100] Train Loss: 0.000076 | Train MSE: 0.000076 | Train MAE: 0.006475 Val MSE: 0.005954 | Val MAE: 0.052216\n",
      "Epoch [93/100] Train Loss: 0.000081 | Train MSE: 0.000081 | Train MAE: 0.006650 Val MSE: 0.005844 | Val MAE: 0.051313\n",
      "Epoch [94/100] Train Loss: 0.000109 | Train MSE: 0.000109 | Train MAE: 0.007489 Val MSE: 0.006138 | Val MAE: 0.053146\n",
      "Epoch [95/100] Train Loss: 0.000145 | Train MSE: 0.000145 | Train MAE: 0.008276 Val MSE: 0.005880 | Val MAE: 0.050967\n",
      "Epoch [96/100] Train Loss: 0.000073 | Train MSE: 0.000073 | Train MAE: 0.006367 Val MSE: 0.005751 | Val MAE: 0.051141\n",
      "Epoch [97/100] Train Loss: 0.000071 | Train MSE: 0.000071 | Train MAE: 0.006271 Val MSE: 0.005674 | Val MAE: 0.050481\n",
      "Epoch [98/100] Train Loss: 0.000071 | Train MSE: 0.000071 | Train MAE: 0.006261 Val MSE: 0.006000 | Val MAE: 0.052402\n",
      "Epoch [99/100] Train Loss: 0.000084 | Train MSE: 0.000084 | Train MAE: 0.006690 Val MSE: 0.005821 | Val MAE: 0.051329\n",
      "Epoch [100/100] Train Loss: 0.000102 | Train MSE: 0.000102 | Train MAE: 0.007282 Val MSE: 0.006332 | Val MAE: 0.053661\n"
     ]
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "        d_model=128, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        num_decoder_layers=4, \n",
    "        dim_feedforward=512, \n",
    "        dropout=0.2).to(device)\n",
    "\n",
    "#损失函数MSE和优化器Adam\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_mse, train_mae = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_mse, val_mae = validate_one_epoch(model, test_loader, criterion, device)\n",
    "    \n",
    "    #保存在验证集上表现最好的模型\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "        torch.save(model.state_dict(), \"trans_best_model1.pth\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.6f} | Train MSE: {train_mse:.6f} | Train MAE: {train_mae:.6f} \"\n",
    "          f\"Val MSE: {val_mse:.6f} | Val MAE: {val_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74eec2c5-93ef-4fc3-8edd-d4c95f6c856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/zczhang/anaconda3/envs/py38lyd/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.056875 | Train MSE: 0.056875 | Train MAE: 0.156554 Val MSE: 0.016402 | Val MAE: 0.090771\n",
      "Epoch [2/100] Train Loss: 0.005871 | Train MSE: 0.005871 | Train MAE: 0.055297 Val MSE: 0.020679 | Val MAE: 0.096666\n",
      "Epoch [3/100] Train Loss: 0.003310 | Train MSE: 0.003310 | Train MAE: 0.041961 Val MSE: 0.011715 | Val MAE: 0.071544\n",
      "Epoch [4/100] Train Loss: 0.002662 | Train MSE: 0.002662 | Train MAE: 0.037333 Val MSE: 0.009184 | Val MAE: 0.065772\n",
      "Epoch [5/100] Train Loss: 0.002259 | Train MSE: 0.002259 | Train MAE: 0.034254 Val MSE: 0.009572 | Val MAE: 0.066887\n",
      "Epoch [6/100] Train Loss: 0.001970 | Train MSE: 0.001970 | Train MAE: 0.031938 Val MSE: 0.009032 | Val MAE: 0.062047\n",
      "Epoch [7/100] Train Loss: 0.001772 | Train MSE: 0.001772 | Train MAE: 0.030274 Val MSE: 0.005112 | Val MAE: 0.047529\n",
      "Epoch [8/100] Train Loss: 0.001619 | Train MSE: 0.001619 | Train MAE: 0.029001 Val MSE: 0.007158 | Val MAE: 0.056488\n",
      "Epoch [9/100] Train Loss: 0.001452 | Train MSE: 0.001452 | Train MAE: 0.027479 Val MSE: 0.005676 | Val MAE: 0.049067\n",
      "Epoch [10/100] Train Loss: 0.001337 | Train MSE: 0.001337 | Train MAE: 0.026385 Val MSE: 0.007361 | Val MAE: 0.060702\n",
      "Epoch [11/100] Train Loss: 0.001226 | Train MSE: 0.001226 | Train MAE: 0.025251 Val MSE: 0.004993 | Val MAE: 0.044394\n",
      "Epoch [12/100] Train Loss: 0.001148 | Train MSE: 0.001148 | Train MAE: 0.024437 Val MSE: 0.007233 | Val MAE: 0.056460\n",
      "Epoch [13/100] Train Loss: 0.001070 | Train MSE: 0.001070 | Train MAE: 0.023589 Val MSE: 0.006892 | Val MAE: 0.055208\n",
      "Epoch [14/100] Train Loss: 0.000982 | Train MSE: 0.000982 | Train MAE: 0.022589 Val MSE: 0.005051 | Val MAE: 0.045138\n",
      "Epoch [15/100] Train Loss: 0.000953 | Train MSE: 0.000953 | Train MAE: 0.022234 Val MSE: 0.006561 | Val MAE: 0.051960\n",
      "Epoch [16/100] Train Loss: 0.000860 | Train MSE: 0.000860 | Train MAE: 0.021137 Val MSE: 0.006982 | Val MAE: 0.055372\n",
      "Epoch [17/100] Train Loss: 0.000820 | Train MSE: 0.000820 | Train MAE: 0.020647 Val MSE: 0.006891 | Val MAE: 0.054927\n",
      "Epoch [18/100] Train Loss: 0.000780 | Train MSE: 0.000780 | Train MAE: 0.020122 Val MSE: 0.004906 | Val MAE: 0.044396\n",
      "Epoch [19/100] Train Loss: 0.000694 | Train MSE: 0.000694 | Train MAE: 0.019037 Val MSE: 0.004986 | Val MAE: 0.044469\n",
      "Epoch [20/100] Train Loss: 0.000641 | Train MSE: 0.000641 | Train MAE: 0.018328 Val MSE: 0.004707 | Val MAE: 0.043500\n",
      "Epoch [21/100] Train Loss: 0.000596 | Train MSE: 0.000596 | Train MAE: 0.017653 Val MSE: 0.004903 | Val MAE: 0.043609\n",
      "Epoch [22/100] Train Loss: 0.000557 | Train MSE: 0.000557 | Train MAE: 0.017051 Val MSE: 0.004224 | Val MAE: 0.039857\n",
      "Epoch [23/100] Train Loss: 0.000585 | Train MSE: 0.000585 | Train MAE: 0.017200 Val MSE: 0.004193 | Val MAE: 0.039605\n",
      "Epoch [24/100] Train Loss: 0.000467 | Train MSE: 0.000467 | Train MAE: 0.015637 Val MSE: 0.004665 | Val MAE: 0.042420\n",
      "Epoch [25/100] Train Loss: 0.000437 | Train MSE: 0.000437 | Train MAE: 0.015169 Val MSE: 0.004853 | Val MAE: 0.042580\n",
      "Epoch [26/100] Train Loss: 0.000404 | Train MSE: 0.000404 | Train MAE: 0.014595 Val MSE: 0.004618 | Val MAE: 0.041626\n",
      "Epoch [27/100] Train Loss: 0.000399 | Train MSE: 0.000399 | Train MAE: 0.014471 Val MSE: 0.004971 | Val MAE: 0.043296\n",
      "Epoch [28/100] Train Loss: 0.000405 | Train MSE: 0.000405 | Train MAE: 0.014522 Val MSE: 0.004283 | Val MAE: 0.039578\n",
      "Epoch [29/100] Train Loss: 0.000535 | Train MSE: 0.000535 | Train MAE: 0.015705 Val MSE: 0.004531 | Val MAE: 0.041603\n",
      "Epoch [30/100] Train Loss: 0.000339 | Train MSE: 0.000339 | Train MAE: 0.013370 Val MSE: 0.004269 | Val MAE: 0.040718\n",
      "Epoch [31/100] Train Loss: 0.000305 | Train MSE: 0.000305 | Train MAE: 0.012735 Val MSE: 0.004468 | Val MAE: 0.040633\n",
      "Epoch [32/100] Train Loss: 0.000301 | Train MSE: 0.000301 | Train MAE: 0.012583 Val MSE: 0.004393 | Val MAE: 0.040065\n",
      "Epoch [33/100] Train Loss: 0.000395 | Train MSE: 0.000395 | Train MAE: 0.013991 Val MSE: 0.004269 | Val MAE: 0.039879\n",
      "Epoch [34/100] Train Loss: 0.000280 | Train MSE: 0.000280 | Train MAE: 0.012165 Val MSE: 0.004788 | Val MAE: 0.042736\n",
      "Epoch [35/100] Train Loss: 0.000269 | Train MSE: 0.000269 | Train MAE: 0.011948 Val MSE: 0.004766 | Val MAE: 0.042943\n",
      "Epoch [36/100] Train Loss: 0.000274 | Train MSE: 0.000274 | Train MAE: 0.012030 Val MSE: 0.004462 | Val MAE: 0.041187\n",
      "Epoch [37/100] Train Loss: 0.000248 | Train MSE: 0.000248 | Train MAE: 0.011508 Val MSE: 0.004612 | Val MAE: 0.041892\n",
      "Epoch [38/100] Train Loss: 0.000244 | Train MSE: 0.000244 | Train MAE: 0.011442 Val MSE: 0.004610 | Val MAE: 0.041646\n",
      "Epoch [39/100] Train Loss: 0.000346 | Train MSE: 0.000346 | Train MAE: 0.013091 Val MSE: 0.004729 | Val MAE: 0.044228\n",
      "Epoch [40/100] Train Loss: 0.000225 | Train MSE: 0.000225 | Train MAE: 0.010937 Val MSE: 0.004343 | Val MAE: 0.040985\n",
      "Epoch [41/100] Train Loss: 0.000208 | Train MSE: 0.000208 | Train MAE: 0.010574 Val MSE: 0.004776 | Val MAE: 0.043545\n",
      "Epoch [42/100] Train Loss: 0.000228 | Train MSE: 0.000228 | Train MAE: 0.010959 Val MSE: 0.004819 | Val MAE: 0.044500\n",
      "Epoch [43/100] Train Loss: 0.000195 | Train MSE: 0.000195 | Train MAE: 0.010255 Val MSE: 0.004738 | Val MAE: 0.043440\n",
      "Epoch [44/100] Train Loss: 0.000199 | Train MSE: 0.000199 | Train MAE: 0.010255 Val MSE: 0.005200 | Val MAE: 0.046707\n",
      "Epoch [45/100] Train Loss: 0.000230 | Train MSE: 0.000230 | Train MAE: 0.010890 Val MSE: 0.005093 | Val MAE: 0.045857\n",
      "Epoch [46/100] Train Loss: 0.000302 | Train MSE: 0.000302 | Train MAE: 0.011914 Val MSE: 0.004733 | Val MAE: 0.043704\n",
      "Epoch [47/100] Train Loss: 0.000178 | Train MSE: 0.000178 | Train MAE: 0.009764 Val MSE: 0.004585 | Val MAE: 0.042777\n",
      "Epoch [48/100] Train Loss: 0.000155 | Train MSE: 0.000155 | Train MAE: 0.009166 Val MSE: 0.004681 | Val MAE: 0.042869\n",
      "Epoch [49/100] Train Loss: 0.000170 | Train MSE: 0.000170 | Train MAE: 0.009493 Val MSE: 0.004572 | Val MAE: 0.043168\n",
      "Epoch [50/100] Train Loss: 0.000173 | Train MSE: 0.000173 | Train MAE: 0.009587 Val MSE: 0.004283 | Val MAE: 0.041649\n",
      "Epoch [51/100] Train Loss: 0.000166 | Train MSE: 0.000166 | Train MAE: 0.009400 Val MSE: 0.004737 | Val MAE: 0.044284\n",
      "Epoch [52/100] Train Loss: 0.000242 | Train MSE: 0.000242 | Train MAE: 0.010451 Val MSE: 0.004556 | Val MAE: 0.043339\n",
      "Epoch [53/100] Train Loss: 0.000197 | Train MSE: 0.000197 | Train MAE: 0.010145 Val MSE: 0.004897 | Val MAE: 0.044711\n",
      "Epoch [54/100] Train Loss: 0.000152 | Train MSE: 0.000152 | Train MAE: 0.009023 Val MSE: 0.004977 | Val MAE: 0.045007\n",
      "Epoch [55/100] Train Loss: 0.000139 | Train MSE: 0.000139 | Train MAE: 0.008663 Val MSE: 0.004747 | Val MAE: 0.043947\n",
      "Epoch [56/100] Train Loss: 0.000158 | Train MSE: 0.000158 | Train MAE: 0.009134 Val MSE: 0.004773 | Val MAE: 0.044221\n",
      "Epoch [57/100] Train Loss: 0.000234 | Train MSE: 0.000234 | Train MAE: 0.010726 Val MSE: 0.004796 | Val MAE: 0.044648\n",
      "Epoch [58/100] Train Loss: 0.000150 | Train MSE: 0.000150 | Train MAE: 0.008969 Val MSE: 0.005218 | Val MAE: 0.046119\n",
      "Epoch [59/100] Train Loss: 0.000139 | Train MSE: 0.000139 | Train MAE: 0.008631 Val MSE: 0.005159 | Val MAE: 0.046156\n",
      "Epoch [60/100] Train Loss: 0.000213 | Train MSE: 0.000213 | Train MAE: 0.010153 Val MSE: 0.004863 | Val MAE: 0.045274\n",
      "Epoch [61/100] Train Loss: 0.000139 | Train MSE: 0.000139 | Train MAE: 0.008606 Val MSE: 0.005042 | Val MAE: 0.045682\n",
      "Epoch [62/100] Train Loss: 0.000123 | Train MSE: 0.000123 | Train MAE: 0.008135 Val MSE: 0.005110 | Val MAE: 0.046169\n",
      "Epoch [63/100] Train Loss: 0.000118 | Train MSE: 0.000118 | Train MAE: 0.007999 Val MSE: 0.004974 | Val MAE: 0.044932\n",
      "Epoch [64/100] Train Loss: 0.000146 | Train MSE: 0.000146 | Train MAE: 0.008658 Val MSE: 0.005040 | Val MAE: 0.046028\n",
      "Epoch [65/100] Train Loss: 0.000161 | Train MSE: 0.000161 | Train MAE: 0.009111 Val MSE: 0.004874 | Val MAE: 0.044685\n",
      "Epoch [66/100] Train Loss: 0.000213 | Train MSE: 0.000213 | Train MAE: 0.009555 Val MSE: 0.005049 | Val MAE: 0.045177\n",
      "Epoch [67/100] Train Loss: 0.000159 | Train MSE: 0.000159 | Train MAE: 0.009064 Val MSE: 0.005481 | Val MAE: 0.048030\n",
      "Epoch [68/100] Train Loss: 0.000123 | Train MSE: 0.000123 | Train MAE: 0.008097 Val MSE: 0.005054 | Val MAE: 0.045808\n",
      "Epoch [69/100] Train Loss: 0.000105 | Train MSE: 0.000105 | Train MAE: 0.007587 Val MSE: 0.005122 | Val MAE: 0.046465\n",
      "Epoch [70/100] Train Loss: 0.000107 | Train MSE: 0.000107 | Train MAE: 0.007634 Val MSE: 0.005704 | Val MAE: 0.049562\n",
      "Epoch [71/100] Train Loss: 0.000116 | Train MSE: 0.000116 | Train MAE: 0.007914 Val MSE: 0.005311 | Val MAE: 0.047522\n",
      "Epoch [72/100] Train Loss: 0.000133 | Train MSE: 0.000133 | Train MAE: 0.008230 Val MSE: 0.005622 | Val MAE: 0.048916\n",
      "Epoch [73/100] Train Loss: 0.000110 | Train MSE: 0.000110 | Train MAE: 0.007692 Val MSE: 0.005019 | Val MAE: 0.045281\n",
      "Epoch [74/100] Train Loss: 0.000108 | Train MSE: 0.000108 | Train MAE: 0.007656 Val MSE: 0.005334 | Val MAE: 0.047278\n",
      "Epoch [75/100] Train Loss: 0.000108 | Train MSE: 0.000108 | Train MAE: 0.007563 Val MSE: 0.005458 | Val MAE: 0.047411\n",
      "Epoch [76/100] Train Loss: 0.000289 | Train MSE: 0.000289 | Train MAE: 0.010819 Val MSE: 0.004814 | Val MAE: 0.044526\n",
      "Epoch [77/100] Train Loss: 0.000136 | Train MSE: 0.000136 | Train MAE: 0.008476 Val MSE: 0.004918 | Val MAE: 0.045452\n",
      "Epoch [78/100] Train Loss: 0.000096 | Train MSE: 0.000096 | Train MAE: 0.007230 Val MSE: 0.004865 | Val MAE: 0.044492\n",
      "Epoch [79/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.006987 Val MSE: 0.004906 | Val MAE: 0.044906\n",
      "Epoch [80/100] Train Loss: 0.000090 | Train MSE: 0.000090 | Train MAE: 0.006980 Val MSE: 0.005157 | Val MAE: 0.045700\n",
      "Epoch [81/100] Train Loss: 0.000092 | Train MSE: 0.000092 | Train MAE: 0.007097 Val MSE: 0.004995 | Val MAE: 0.044890\n",
      "Epoch [82/100] Train Loss: 0.000090 | Train MSE: 0.000090 | Train MAE: 0.007017 Val MSE: 0.005844 | Val MAE: 0.050639\n",
      "Epoch [83/100] Train Loss: 0.000325 | Train MSE: 0.000325 | Train MAE: 0.011663 Val MSE: 0.005239 | Val MAE: 0.046018\n",
      "Epoch [84/100] Train Loss: 0.000118 | Train MSE: 0.000118 | Train MAE: 0.007873 Val MSE: 0.005134 | Val MAE: 0.045922\n",
      "Epoch [85/100] Train Loss: 0.000091 | Train MSE: 0.000091 | Train MAE: 0.007035 Val MSE: 0.005164 | Val MAE: 0.046125\n",
      "Epoch [86/100] Train Loss: 0.000084 | Train MSE: 0.000084 | Train MAE: 0.006794 Val MSE: 0.005353 | Val MAE: 0.047376\n",
      "Epoch [87/100] Train Loss: 0.000084 | Train MSE: 0.000084 | Train MAE: 0.006787 Val MSE: 0.005234 | Val MAE: 0.046175\n",
      "Epoch [88/100] Train Loss: 0.000096 | Train MSE: 0.000096 | Train MAE: 0.007183 Val MSE: 0.004980 | Val MAE: 0.045345\n",
      "Epoch [89/100] Train Loss: 0.000085 | Train MSE: 0.000085 | Train MAE: 0.006797 Val MSE: 0.005225 | Val MAE: 0.045695\n",
      "Epoch [90/100] Train Loss: 0.000096 | Train MSE: 0.000096 | Train MAE: 0.007157 Val MSE: 0.005082 | Val MAE: 0.045166\n",
      "Epoch [91/100] Train Loss: 0.000093 | Train MSE: 0.000093 | Train MAE: 0.007047 Val MSE: 0.005127 | Val MAE: 0.047132\n",
      "Epoch [92/100] Train Loss: 0.000094 | Train MSE: 0.000094 | Train MAE: 0.007108 Val MSE: 0.005346 | Val MAE: 0.046437\n",
      "Epoch [93/100] Train Loss: 0.000136 | Train MSE: 0.000136 | Train MAE: 0.008086 Val MSE: 0.005440 | Val MAE: 0.047864\n",
      "Epoch [94/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.006942 Val MSE: 0.005344 | Val MAE: 0.046638\n",
      "Epoch [95/100] Train Loss: 0.000110 | Train MSE: 0.000110 | Train MAE: 0.007411 Val MSE: 0.005431 | Val MAE: 0.048048\n",
      "Epoch [96/100] Train Loss: 0.000096 | Train MSE: 0.000096 | Train MAE: 0.007142 Val MSE: 0.005238 | Val MAE: 0.045991\n",
      "Epoch [97/100] Train Loss: 0.000077 | Train MSE: 0.000077 | Train MAE: 0.006492 Val MSE: 0.005467 | Val MAE: 0.047097\n",
      "Epoch [98/100] Train Loss: 0.000073 | Train MSE: 0.000073 | Train MAE: 0.006328 Val MSE: 0.005265 | Val MAE: 0.046882\n",
      "Epoch [99/100] Train Loss: 0.000090 | Train MSE: 0.000090 | Train MAE: 0.006922 Val MSE: 0.005073 | Val MAE: 0.045581\n",
      "Epoch [100/100] Train Loss: 0.000129 | Train MSE: 0.000129 | Train MAE: 0.007858 Val MSE: 0.004904 | Val MAE: 0.045095\n"
     ]
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "        d_model=128, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        num_decoder_layers=4, \n",
    "        dim_feedforward=256, \n",
    "        dropout=0.2).to(device)\n",
    "\n",
    "#损失函数MSE和优化器Adam\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_mse, train_mae = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_mse, val_mae = validate_one_epoch(model, test_loader, criterion, device)\n",
    "    \n",
    "    #保存在验证集上表现最好的模型\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "        torch.save(model.state_dict(), \"trans_best_model2.pth\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.6f} | Train MSE: {train_mse:.6f} | Train MAE: {train_mae:.6f} \"\n",
    "          f\"Val MSE: {val_mse:.6f} | Val MAE: {val_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7734fe92-776e-458c-bf73-14777be0a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.061028 | Train MSE: 0.061028 | Train MAE: 0.155206 Val MSE: 0.028932 | Val MAE: 0.114378\n",
      "Epoch [2/100] Train Loss: 0.005916 | Train MSE: 0.005916 | Train MAE: 0.056801 Val MSE: 0.021173 | Val MAE: 0.103883\n",
      "Epoch [3/100] Train Loss: 0.003677 | Train MSE: 0.003677 | Train MAE: 0.044541 Val MSE: 0.015567 | Val MAE: 0.088730\n",
      "Epoch [4/100] Train Loss: 0.002955 | Train MSE: 0.002955 | Train MAE: 0.039547 Val MSE: 0.012401 | Val MAE: 0.074657\n",
      "Epoch [5/100] Train Loss: 0.002521 | Train MSE: 0.002521 | Train MAE: 0.036382 Val MSE: 0.008487 | Val MAE: 0.061266\n",
      "Epoch [6/100] Train Loss: 0.002221 | Train MSE: 0.002221 | Train MAE: 0.034076 Val MSE: 0.011557 | Val MAE: 0.072858\n",
      "Epoch [7/100] Train Loss: 0.001969 | Train MSE: 0.001969 | Train MAE: 0.032066 Val MSE: 0.009411 | Val MAE: 0.064405\n",
      "Epoch [8/100] Train Loss: 0.001759 | Train MSE: 0.001759 | Train MAE: 0.030281 Val MSE: 0.009583 | Val MAE: 0.063150\n",
      "Epoch [9/100] Train Loss: 0.001608 | Train MSE: 0.001608 | Train MAE: 0.028949 Val MSE: 0.009558 | Val MAE: 0.063505\n",
      "Epoch [10/100] Train Loss: 0.001464 | Train MSE: 0.001464 | Train MAE: 0.027606 Val MSE: 0.009216 | Val MAE: 0.063991\n",
      "Epoch [11/100] Train Loss: 0.001362 | Train MSE: 0.001362 | Train MAE: 0.026649 Val MSE: 0.008336 | Val MAE: 0.061211\n",
      "Epoch [12/100] Train Loss: 0.001262 | Train MSE: 0.001262 | Train MAE: 0.025637 Val MSE: 0.009480 | Val MAE: 0.066231\n",
      "Epoch [13/100] Train Loss: 0.001171 | Train MSE: 0.001171 | Train MAE: 0.024703 Val MSE: 0.010555 | Val MAE: 0.069253\n",
      "Epoch [14/100] Train Loss: 0.001100 | Train MSE: 0.001100 | Train MAE: 0.023883 Val MSE: 0.008681 | Val MAE: 0.059703\n",
      "Epoch [15/100] Train Loss: 0.001198 | Train MSE: 0.001198 | Train MAE: 0.024512 Val MSE: 0.008927 | Val MAE: 0.061451\n",
      "Epoch [16/100] Train Loss: 0.000968 | Train MSE: 0.000968 | Train MAE: 0.022357 Val MSE: 0.007894 | Val MAE: 0.057586\n",
      "Epoch [17/100] Train Loss: 0.000898 | Train MSE: 0.000898 | Train MAE: 0.021550 Val MSE: 0.007529 | Val MAE: 0.057056\n",
      "Epoch [18/100] Train Loss: 0.000812 | Train MSE: 0.000812 | Train MAE: 0.020501 Val MSE: 0.006407 | Val MAE: 0.051743\n",
      "Epoch [19/100] Train Loss: 0.000762 | Train MSE: 0.000762 | Train MAE: 0.019864 Val MSE: 0.005952 | Val MAE: 0.048469\n",
      "Epoch [20/100] Train Loss: 0.000696 | Train MSE: 0.000696 | Train MAE: 0.018993 Val MSE: 0.005641 | Val MAE: 0.046666\n",
      "Epoch [21/100] Train Loss: 0.000627 | Train MSE: 0.000627 | Train MAE: 0.018022 Val MSE: 0.005714 | Val MAE: 0.048244\n",
      "Epoch [22/100] Train Loss: 0.000571 | Train MSE: 0.000571 | Train MAE: 0.017233 Val MSE: 0.004892 | Val MAE: 0.043483\n",
      "Epoch [23/100] Train Loss: 0.000542 | Train MSE: 0.000542 | Train MAE: 0.016815 Val MSE: 0.005229 | Val MAE: 0.044857\n",
      "Epoch [24/100] Train Loss: 0.000618 | Train MSE: 0.000618 | Train MAE: 0.017422 Val MSE: 0.004544 | Val MAE: 0.041440\n",
      "Epoch [25/100] Train Loss: 0.000475 | Train MSE: 0.000475 | Train MAE: 0.015730 Val MSE: 0.004743 | Val MAE: 0.042721\n",
      "Epoch [26/100] Train Loss: 0.000446 | Train MSE: 0.000446 | Train MAE: 0.015293 Val MSE: 0.004738 | Val MAE: 0.041579\n",
      "Epoch [27/100] Train Loss: 0.000428 | Train MSE: 0.000428 | Train MAE: 0.014971 Val MSE: 0.004819 | Val MAE: 0.042017\n",
      "Epoch [28/100] Train Loss: 0.000405 | Train MSE: 0.000405 | Train MAE: 0.014586 Val MSE: 0.004639 | Val MAE: 0.041280\n",
      "Epoch [29/100] Train Loss: 0.000391 | Train MSE: 0.000391 | Train MAE: 0.014284 Val MSE: 0.004503 | Val MAE: 0.040999\n",
      "Epoch [30/100] Train Loss: 0.000356 | Train MSE: 0.000356 | Train MAE: 0.013724 Val MSE: 0.004942 | Val MAE: 0.044296\n",
      "Epoch [31/100] Train Loss: 0.000356 | Train MSE: 0.000356 | Train MAE: 0.013698 Val MSE: 0.004471 | Val MAE: 0.041605\n",
      "Epoch [32/100] Train Loss: 0.000320 | Train MSE: 0.000320 | Train MAE: 0.013043 Val MSE: 0.004675 | Val MAE: 0.042180\n",
      "Epoch [33/100] Train Loss: 0.000384 | Train MSE: 0.000384 | Train MAE: 0.013828 Val MSE: 0.004974 | Val MAE: 0.046570\n",
      "Epoch [34/100] Train Loss: 0.000350 | Train MSE: 0.000350 | Train MAE: 0.013400 Val MSE: 0.004797 | Val MAE: 0.043438\n",
      "Epoch [35/100] Train Loss: 0.000271 | Train MSE: 0.000271 | Train MAE: 0.011995 Val MSE: 0.004630 | Val MAE: 0.042970\n",
      "Epoch [36/100] Train Loss: 0.000263 | Train MSE: 0.000263 | Train MAE: 0.011856 Val MSE: 0.004496 | Val MAE: 0.041516\n",
      "Epoch [37/100] Train Loss: 0.000249 | Train MSE: 0.000249 | Train MAE: 0.011533 Val MSE: 0.004700 | Val MAE: 0.043205\n",
      "Epoch [38/100] Train Loss: 0.000282 | Train MSE: 0.000282 | Train MAE: 0.012173 Val MSE: 0.004527 | Val MAE: 0.042323\n",
      "Epoch [39/100] Train Loss: 0.000255 | Train MSE: 0.000255 | Train MAE: 0.011592 Val MSE: 0.004499 | Val MAE: 0.041710\n",
      "Epoch [40/100] Train Loss: 0.000239 | Train MSE: 0.000239 | Train MAE: 0.011261 Val MSE: 0.004572 | Val MAE: 0.043174\n",
      "Epoch [41/100] Train Loss: 0.000249 | Train MSE: 0.000249 | Train MAE: 0.011406 Val MSE: 0.004855 | Val MAE: 0.043633\n",
      "Epoch [42/100] Train Loss: 0.000215 | Train MSE: 0.000215 | Train MAE: 0.010757 Val MSE: 0.004583 | Val MAE: 0.042568\n",
      "Epoch [43/100] Train Loss: 0.000206 | Train MSE: 0.000206 | Train MAE: 0.010520 Val MSE: 0.004237 | Val MAE: 0.040487\n",
      "Epoch [44/100] Train Loss: 0.000184 | Train MSE: 0.000184 | Train MAE: 0.009980 Val MSE: 0.004382 | Val MAE: 0.041419\n",
      "Epoch [45/100] Train Loss: 0.000402 | Train MSE: 0.000402 | Train MAE: 0.013435 Val MSE: 0.004640 | Val MAE: 0.043018\n",
      "Epoch [46/100] Train Loss: 0.000209 | Train MSE: 0.000209 | Train MAE: 0.010580 Val MSE: 0.004628 | Val MAE: 0.042825\n",
      "Epoch [47/100] Train Loss: 0.000184 | Train MSE: 0.000184 | Train MAE: 0.009989 Val MSE: 0.004397 | Val MAE: 0.041828\n",
      "Epoch [48/100] Train Loss: 0.000181 | Train MSE: 0.000181 | Train MAE: 0.009896 Val MSE: 0.004581 | Val MAE: 0.042553\n",
      "Epoch [49/100] Train Loss: 0.000174 | Train MSE: 0.000174 | Train MAE: 0.009716 Val MSE: 0.004737 | Val MAE: 0.043701\n",
      "Epoch [50/100] Train Loss: 0.000164 | Train MSE: 0.000164 | Train MAE: 0.009409 Val MSE: 0.004452 | Val MAE: 0.042504\n",
      "Epoch [51/100] Train Loss: 0.000189 | Train MSE: 0.000189 | Train MAE: 0.010025 Val MSE: 0.004803 | Val MAE: 0.044572\n",
      "Epoch [52/100] Train Loss: 0.000282 | Train MSE: 0.000282 | Train MAE: 0.011343 Val MSE: 0.004848 | Val MAE: 0.045109\n",
      "Epoch [53/100] Train Loss: 0.000189 | Train MSE: 0.000189 | Train MAE: 0.010026 Val MSE: 0.004968 | Val MAE: 0.046010\n",
      "Epoch [54/100] Train Loss: 0.000157 | Train MSE: 0.000157 | Train MAE: 0.009230 Val MSE: 0.004923 | Val MAE: 0.045523\n",
      "Epoch [55/100] Train Loss: 0.000154 | Train MSE: 0.000154 | Train MAE: 0.009128 Val MSE: 0.004738 | Val MAE: 0.043948\n",
      "Epoch [56/100] Train Loss: 0.000148 | Train MSE: 0.000148 | Train MAE: 0.008963 Val MSE: 0.004660 | Val MAE: 0.043915\n",
      "Epoch [57/100] Train Loss: 0.000147 | Train MSE: 0.000147 | Train MAE: 0.008919 Val MSE: 0.004930 | Val MAE: 0.045328\n",
      "Epoch [58/100] Train Loss: 0.000141 | Train MSE: 0.000141 | Train MAE: 0.008740 Val MSE: 0.004946 | Val MAE: 0.045656\n",
      "Epoch [59/100] Train Loss: 0.000186 | Train MSE: 0.000186 | Train MAE: 0.009772 Val MSE: 0.004822 | Val MAE: 0.045341\n",
      "Epoch [60/100] Train Loss: 0.000159 | Train MSE: 0.000159 | Train MAE: 0.009216 Val MSE: 0.004676 | Val MAE: 0.044599\n",
      "Epoch [61/100] Train Loss: 0.000154 | Train MSE: 0.000154 | Train MAE: 0.008984 Val MSE: 0.004839 | Val MAE: 0.044937\n",
      "Epoch [62/100] Train Loss: 0.000236 | Train MSE: 0.000236 | Train MAE: 0.010406 Val MSE: 0.004738 | Val MAE: 0.044295\n",
      "Epoch [63/100] Train Loss: 0.000140 | Train MSE: 0.000140 | Train MAE: 0.008696 Val MSE: 0.004691 | Val MAE: 0.043911\n",
      "Epoch [64/100] Train Loss: 0.000132 | Train MSE: 0.000132 | Train MAE: 0.008404 Val MSE: 0.004768 | Val MAE: 0.044662\n",
      "Epoch [65/100] Train Loss: 0.000134 | Train MSE: 0.000134 | Train MAE: 0.008463 Val MSE: 0.004993 | Val MAE: 0.045793\n",
      "Epoch [66/100] Train Loss: 0.000125 | Train MSE: 0.000125 | Train MAE: 0.008230 Val MSE: 0.004891 | Val MAE: 0.045416\n",
      "Epoch [67/100] Train Loss: 0.000114 | Train MSE: 0.000114 | Train MAE: 0.007933 Val MSE: 0.004905 | Val MAE: 0.045407\n",
      "Epoch [68/100] Train Loss: 0.000123 | Train MSE: 0.000123 | Train MAE: 0.008191 Val MSE: 0.004713 | Val MAE: 0.045044\n",
      "Epoch [69/100] Train Loss: 0.000121 | Train MSE: 0.000121 | Train MAE: 0.008086 Val MSE: 0.004930 | Val MAE: 0.045279\n",
      "Epoch [70/100] Train Loss: 0.000149 | Train MSE: 0.000149 | Train MAE: 0.008845 Val MSE: 0.005043 | Val MAE: 0.045763\n",
      "Epoch [71/100] Train Loss: 0.000123 | Train MSE: 0.000123 | Train MAE: 0.008077 Val MSE: 0.005187 | Val MAE: 0.046951\n",
      "Epoch [72/100] Train Loss: 0.000167 | Train MSE: 0.000167 | Train MAE: 0.009158 Val MSE: 0.004785 | Val MAE: 0.045070\n",
      "Epoch [73/100] Train Loss: 0.000102 | Train MSE: 0.000102 | Train MAE: 0.007508 Val MSE: 0.004832 | Val MAE: 0.045288\n",
      "Epoch [74/100] Train Loss: 0.000106 | Train MSE: 0.000106 | Train MAE: 0.007631 Val MSE: 0.004464 | Val MAE: 0.042547\n",
      "Epoch [75/100] Train Loss: 0.000129 | Train MSE: 0.000129 | Train MAE: 0.008168 Val MSE: 0.005508 | Val MAE: 0.049477\n",
      "Epoch [76/100] Train Loss: 0.000110 | Train MSE: 0.000110 | Train MAE: 0.007727 Val MSE: 0.004805 | Val MAE: 0.045084\n",
      "Epoch [77/100] Train Loss: 0.000118 | Train MSE: 0.000118 | Train MAE: 0.007899 Val MSE: 0.005097 | Val MAE: 0.047195\n",
      "Epoch [78/100] Train Loss: 0.000178 | Train MSE: 0.000178 | Train MAE: 0.009117 Val MSE: 0.004897 | Val MAE: 0.045350\n",
      "Epoch [79/100] Train Loss: 0.000093 | Train MSE: 0.000093 | Train MAE: 0.007165 Val MSE: 0.004858 | Val MAE: 0.044879\n",
      "Epoch [80/100] Train Loss: 0.000092 | Train MSE: 0.000092 | Train MAE: 0.007133 Val MSE: 0.004769 | Val MAE: 0.044350\n",
      "Epoch [81/100] Train Loss: 0.000130 | Train MSE: 0.000130 | Train MAE: 0.008123 Val MSE: 0.004932 | Val MAE: 0.045814\n",
      "Epoch [82/100] Train Loss: 0.000094 | Train MSE: 0.000094 | Train MAE: 0.007176 Val MSE: 0.004956 | Val MAE: 0.045834\n",
      "Epoch [83/100] Train Loss: 0.000099 | Train MSE: 0.000099 | Train MAE: 0.007324 Val MSE: 0.005105 | Val MAE: 0.047041\n",
      "Epoch [84/100] Train Loss: 0.000088 | Train MSE: 0.000088 | Train MAE: 0.006958 Val MSE: 0.004969 | Val MAE: 0.045733\n",
      "Epoch [85/100] Train Loss: 0.000095 | Train MSE: 0.000095 | Train MAE: 0.007207 Val MSE: 0.004874 | Val MAE: 0.045293\n",
      "Epoch [86/100] Train Loss: 0.000103 | Train MSE: 0.000103 | Train MAE: 0.007410 Val MSE: 0.004991 | Val MAE: 0.046223\n",
      "Epoch [87/100] Train Loss: 0.000092 | Train MSE: 0.000092 | Train MAE: 0.007088 Val MSE: 0.005202 | Val MAE: 0.047158\n",
      "Epoch [88/100] Train Loss: 0.000101 | Train MSE: 0.000101 | Train MAE: 0.007369 Val MSE: 0.005809 | Val MAE: 0.050745\n",
      "Epoch [89/100] Train Loss: 0.000088 | Train MSE: 0.000088 | Train MAE: 0.006984 Val MSE: 0.005119 | Val MAE: 0.046928\n",
      "Epoch [90/100] Train Loss: 0.000084 | Train MSE: 0.000084 | Train MAE: 0.006817 Val MSE: 0.004980 | Val MAE: 0.045875\n",
      "Epoch [91/100] Train Loss: 0.000166 | Train MSE: 0.000166 | Train MAE: 0.008871 Val MSE: 0.005314 | Val MAE: 0.047919\n",
      "Epoch [92/100] Train Loss: 0.000092 | Train MSE: 0.000092 | Train MAE: 0.007102 Val MSE: 0.005023 | Val MAE: 0.045872\n",
      "Epoch [93/100] Train Loss: 0.000079 | Train MSE: 0.000079 | Train MAE: 0.006637 Val MSE: 0.004998 | Val MAE: 0.045542\n",
      "Epoch [94/100] Train Loss: 0.000078 | Train MSE: 0.000078 | Train MAE: 0.006579 Val MSE: 0.004893 | Val MAE: 0.045293\n",
      "Epoch [95/100] Train Loss: 0.000079 | Train MSE: 0.000079 | Train MAE: 0.006591 Val MSE: 0.005004 | Val MAE: 0.046418\n",
      "Epoch [96/100] Train Loss: 0.000109 | Train MSE: 0.000109 | Train MAE: 0.007481 Val MSE: 0.005912 | Val MAE: 0.049915\n",
      "Epoch [97/100] Train Loss: 0.000098 | Train MSE: 0.000098 | Train MAE: 0.007190 Val MSE: 0.005022 | Val MAE: 0.046103\n",
      "Epoch [98/100] Train Loss: 0.000122 | Train MSE: 0.000122 | Train MAE: 0.007771 Val MSE: 0.004966 | Val MAE: 0.045785\n",
      "Epoch [99/100] Train Loss: 0.000101 | Train MSE: 0.000101 | Train MAE: 0.007194 Val MSE: 0.005011 | Val MAE: 0.045799\n",
      "Epoch [100/100] Train Loss: 0.000078 | Train MSE: 0.000078 | Train MAE: 0.006563 Val MSE: 0.005134 | Val MAE: 0.046639\n"
     ]
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "        d_model=128, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        num_decoder_layers=4, \n",
    "        dim_feedforward=256, \n",
    "        dropout=0.2).to(device)\n",
    "\n",
    "#损失函数MSE和优化器Adam\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_mse, train_mae = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_mse, val_mae = validate_one_epoch(model, test_loader, criterion, device)\n",
    "    \n",
    "    #保存在验证集上表现最好的模型\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "        torch.save(model.state_dict(), \"trans_best_model3.pth\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.6f} | Train MSE: {train_mse:.6f} | Train MAE: {train_mae:.6f} \"\n",
    "          f\"Val MSE: {val_mse:.6f} | Val MAE: {val_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4aea17e5-8667-4f35-aeeb-3eeadfa18b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.064076 | Train MSE: 0.064076 | Train MAE: 0.163963 Val MSE: 0.022346 | Val MAE: 0.112223\n",
      "Epoch [2/100] Train Loss: 0.006738 | Train MSE: 0.006738 | Train MAE: 0.058782 Val MSE: 0.008858 | Val MAE: 0.062068\n",
      "Epoch [3/100] Train Loss: 0.003365 | Train MSE: 0.003365 | Train MAE: 0.042384 Val MSE: 0.008983 | Val MAE: 0.061690\n",
      "Epoch [4/100] Train Loss: 0.002643 | Train MSE: 0.002643 | Train MAE: 0.037283 Val MSE: 0.008373 | Val MAE: 0.059327\n",
      "Epoch [5/100] Train Loss: 0.002252 | Train MSE: 0.002252 | Train MAE: 0.034310 Val MSE: 0.007131 | Val MAE: 0.053335\n",
      "Epoch [6/100] Train Loss: 0.001934 | Train MSE: 0.001934 | Train MAE: 0.031757 Val MSE: 0.007809 | Val MAE: 0.056217\n",
      "Epoch [7/100] Train Loss: 0.001730 | Train MSE: 0.001730 | Train MAE: 0.030025 Val MSE: 0.007548 | Val MAE: 0.057641\n",
      "Epoch [8/100] Train Loss: 0.001530 | Train MSE: 0.001530 | Train MAE: 0.028269 Val MSE: 0.006631 | Val MAE: 0.052490\n",
      "Epoch [9/100] Train Loss: 0.001395 | Train MSE: 0.001395 | Train MAE: 0.027013 Val MSE: 0.006554 | Val MAE: 0.053269\n",
      "Epoch [10/100] Train Loss: 0.001273 | Train MSE: 0.001273 | Train MAE: 0.025831 Val MSE: 0.005558 | Val MAE: 0.047890\n",
      "Epoch [11/100] Train Loss: 0.001198 | Train MSE: 0.001198 | Train MAE: 0.025113 Val MSE: 0.005751 | Val MAE: 0.048812\n",
      "Epoch [12/100] Train Loss: 0.001089 | Train MSE: 0.001089 | Train MAE: 0.023909 Val MSE: 0.005581 | Val MAE: 0.047984\n",
      "Epoch [13/100] Train Loss: 0.001059 | Train MSE: 0.001059 | Train MAE: 0.023437 Val MSE: 0.004504 | Val MAE: 0.042977\n",
      "Epoch [14/100] Train Loss: 0.000945 | Train MSE: 0.000945 | Train MAE: 0.022271 Val MSE: 0.004418 | Val MAE: 0.041298\n",
      "Epoch [15/100] Train Loss: 0.000869 | Train MSE: 0.000869 | Train MAE: 0.021431 Val MSE: 0.004752 | Val MAE: 0.042085\n",
      "Epoch [16/100] Train Loss: 0.000802 | Train MSE: 0.000802 | Train MAE: 0.020596 Val MSE: 0.004427 | Val MAE: 0.041866\n",
      "Epoch [17/100] Train Loss: 0.000762 | Train MSE: 0.000762 | Train MAE: 0.020101 Val MSE: 0.004685 | Val MAE: 0.043260\n",
      "Epoch [18/100] Train Loss: 0.000692 | Train MSE: 0.000692 | Train MAE: 0.019144 Val MSE: 0.005007 | Val MAE: 0.044300\n",
      "Epoch [19/100] Train Loss: 0.000659 | Train MSE: 0.000659 | Train MAE: 0.018626 Val MSE: 0.003795 | Val MAE: 0.038148\n",
      "Epoch [20/100] Train Loss: 0.000619 | Train MSE: 0.000619 | Train MAE: 0.018004 Val MSE: 0.004249 | Val MAE: 0.040208\n",
      "Epoch [21/100] Train Loss: 0.000563 | Train MSE: 0.000563 | Train MAE: 0.017232 Val MSE: 0.004061 | Val MAE: 0.039676\n",
      "Epoch [22/100] Train Loss: 0.000514 | Train MSE: 0.000514 | Train MAE: 0.016465 Val MSE: 0.003967 | Val MAE: 0.039296\n",
      "Epoch [23/100] Train Loss: 0.000496 | Train MSE: 0.000496 | Train MAE: 0.016084 Val MSE: 0.004359 | Val MAE: 0.041719\n",
      "Epoch [24/100] Train Loss: 0.000478 | Train MSE: 0.000478 | Train MAE: 0.015881 Val MSE: 0.004139 | Val MAE: 0.039902\n",
      "Epoch [25/100] Train Loss: 0.000430 | Train MSE: 0.000430 | Train MAE: 0.015088 Val MSE: 0.004945 | Val MAE: 0.045126\n",
      "Epoch [26/100] Train Loss: 0.000402 | Train MSE: 0.000402 | Train MAE: 0.014575 Val MSE: 0.004094 | Val MAE: 0.040532\n",
      "Epoch [27/100] Train Loss: 0.000400 | Train MSE: 0.000400 | Train MAE: 0.014481 Val MSE: 0.005147 | Val MAE: 0.045771\n",
      "Epoch [28/100] Train Loss: 0.000376 | Train MSE: 0.000376 | Train MAE: 0.014076 Val MSE: 0.004203 | Val MAE: 0.040496\n",
      "Epoch [29/100] Train Loss: 0.000399 | Train MSE: 0.000399 | Train MAE: 0.014323 Val MSE: 0.004546 | Val MAE: 0.043966\n",
      "Epoch [30/100] Train Loss: 0.000313 | Train MSE: 0.000313 | Train MAE: 0.012928 Val MSE: 0.005637 | Val MAE: 0.048150\n",
      "Epoch [31/100] Train Loss: 0.000319 | Train MSE: 0.000319 | Train MAE: 0.013023 Val MSE: 0.003672 | Val MAE: 0.037477\n",
      "Epoch [32/100] Train Loss: 0.000475 | Train MSE: 0.000475 | Train MAE: 0.015045 Val MSE: 0.004295 | Val MAE: 0.041961\n",
      "Epoch [33/100] Train Loss: 0.000290 | Train MSE: 0.000290 | Train MAE: 0.012466 Val MSE: 0.003998 | Val MAE: 0.039978\n",
      "Epoch [34/100] Train Loss: 0.000268 | Train MSE: 0.000268 | Train MAE: 0.011968 Val MSE: 0.004526 | Val MAE: 0.042593\n",
      "Epoch [35/100] Train Loss: 0.000299 | Train MSE: 0.000299 | Train MAE: 0.012565 Val MSE: 0.004078 | Val MAE: 0.039645\n",
      "Epoch [36/100] Train Loss: 0.000257 | Train MSE: 0.000257 | Train MAE: 0.011740 Val MSE: 0.003872 | Val MAE: 0.039634\n",
      "Epoch [37/100] Train Loss: 0.000249 | Train MSE: 0.000249 | Train MAE: 0.011558 Val MSE: 0.003766 | Val MAE: 0.039003\n",
      "Epoch [38/100] Train Loss: 0.000230 | Train MSE: 0.000230 | Train MAE: 0.011125 Val MSE: 0.003864 | Val MAE: 0.039416\n",
      "Epoch [39/100] Train Loss: 0.000261 | Train MSE: 0.000261 | Train MAE: 0.011767 Val MSE: 0.004064 | Val MAE: 0.041264\n",
      "Epoch [40/100] Train Loss: 0.000242 | Train MSE: 0.000242 | Train MAE: 0.011350 Val MSE: 0.004150 | Val MAE: 0.040283\n",
      "Epoch [41/100] Train Loss: 0.000224 | Train MSE: 0.000224 | Train MAE: 0.010919 Val MSE: 0.003866 | Val MAE: 0.039822\n",
      "Epoch [42/100] Train Loss: 0.000218 | Train MSE: 0.000218 | Train MAE: 0.010795 Val MSE: 0.003939 | Val MAE: 0.040026\n",
      "Epoch [43/100] Train Loss: 0.000230 | Train MSE: 0.000230 | Train MAE: 0.011008 Val MSE: 0.003810 | Val MAE: 0.039433\n",
      "Epoch [44/100] Train Loss: 0.000199 | Train MSE: 0.000199 | Train MAE: 0.010357 Val MSE: 0.004707 | Val MAE: 0.042581\n",
      "Epoch [45/100] Train Loss: 0.000313 | Train MSE: 0.000313 | Train MAE: 0.012097 Val MSE: 0.003673 | Val MAE: 0.038967\n",
      "Epoch [46/100] Train Loss: 0.000182 | Train MSE: 0.000182 | Train MAE: 0.009918 Val MSE: 0.003974 | Val MAE: 0.040562\n",
      "Epoch [47/100] Train Loss: 0.000181 | Train MSE: 0.000181 | Train MAE: 0.009856 Val MSE: 0.004066 | Val MAE: 0.040710\n",
      "Epoch [48/100] Train Loss: 0.000183 | Train MSE: 0.000183 | Train MAE: 0.009909 Val MSE: 0.003789 | Val MAE: 0.039163\n",
      "Epoch [49/100] Train Loss: 0.000199 | Train MSE: 0.000199 | Train MAE: 0.010302 Val MSE: 0.004044 | Val MAE: 0.040623\n",
      "Epoch [50/100] Train Loss: 0.000180 | Train MSE: 0.000180 | Train MAE: 0.009808 Val MSE: 0.004268 | Val MAE: 0.042335\n",
      "Epoch [51/100] Train Loss: 0.000212 | Train MSE: 0.000212 | Train MAE: 0.010490 Val MSE: 0.004471 | Val MAE: 0.042902\n",
      "Epoch [52/100] Train Loss: 0.000161 | Train MSE: 0.000161 | Train MAE: 0.009325 Val MSE: 0.004501 | Val MAE: 0.043155\n",
      "Epoch [53/100] Train Loss: 0.000169 | Train MSE: 0.000169 | Train MAE: 0.009487 Val MSE: 0.004619 | Val MAE: 0.044367\n",
      "Epoch [54/100] Train Loss: 0.000159 | Train MSE: 0.000159 | Train MAE: 0.009274 Val MSE: 0.004196 | Val MAE: 0.040751\n",
      "Epoch [55/100] Train Loss: 0.000165 | Train MSE: 0.000165 | Train MAE: 0.009357 Val MSE: 0.004094 | Val MAE: 0.041379\n",
      "Epoch [56/100] Train Loss: 0.000153 | Train MSE: 0.000153 | Train MAE: 0.009091 Val MSE: 0.004059 | Val MAE: 0.041214\n",
      "Epoch [57/100] Train Loss: 0.000176 | Train MSE: 0.000176 | Train MAE: 0.009575 Val MSE: 0.004570 | Val MAE: 0.042961\n",
      "Epoch [58/100] Train Loss: 0.000173 | Train MSE: 0.000173 | Train MAE: 0.009691 Val MSE: 0.004096 | Val MAE: 0.040975\n",
      "Epoch [59/100] Train Loss: 0.000148 | Train MSE: 0.000148 | Train MAE: 0.008983 Val MSE: 0.004031 | Val MAE: 0.040953\n",
      "Epoch [60/100] Train Loss: 0.000167 | Train MSE: 0.000167 | Train MAE: 0.009408 Val MSE: 0.004507 | Val MAE: 0.042658\n",
      "Epoch [61/100] Train Loss: 0.000216 | Train MSE: 0.000216 | Train MAE: 0.010432 Val MSE: 0.004181 | Val MAE: 0.041745\n",
      "Epoch [62/100] Train Loss: 0.000138 | Train MSE: 0.000138 | Train MAE: 0.008686 Val MSE: 0.004163 | Val MAE: 0.041090\n",
      "Epoch [63/100] Train Loss: 0.000129 | Train MSE: 0.000129 | Train MAE: 0.008422 Val MSE: 0.004549 | Val MAE: 0.043670\n",
      "Epoch [64/100] Train Loss: 0.000146 | Train MSE: 0.000146 | Train MAE: 0.008832 Val MSE: 0.004161 | Val MAE: 0.041121\n",
      "Epoch [65/100] Train Loss: 0.000145 | Train MSE: 0.000145 | Train MAE: 0.008787 Val MSE: 0.004101 | Val MAE: 0.040096\n",
      "Epoch [66/100] Train Loss: 0.000136 | Train MSE: 0.000136 | Train MAE: 0.008535 Val MSE: 0.004385 | Val MAE: 0.042527\n",
      "Epoch [67/100] Train Loss: 0.000151 | Train MSE: 0.000151 | Train MAE: 0.008915 Val MSE: 0.004581 | Val MAE: 0.043356\n",
      "Epoch [68/100] Train Loss: 0.000128 | Train MSE: 0.000128 | Train MAE: 0.008305 Val MSE: 0.004002 | Val MAE: 0.039774\n",
      "Epoch [69/100] Train Loss: 0.000166 | Train MSE: 0.000166 | Train MAE: 0.009275 Val MSE: 0.004202 | Val MAE: 0.041285\n",
      "Epoch [70/100] Train Loss: 0.000122 | Train MSE: 0.000122 | Train MAE: 0.008164 Val MSE: 0.004101 | Val MAE: 0.041288\n",
      "Epoch [71/100] Train Loss: 0.000121 | Train MSE: 0.000121 | Train MAE: 0.008117 Val MSE: 0.004443 | Val MAE: 0.043185\n",
      "Epoch [72/100] Train Loss: 0.000150 | Train MSE: 0.000150 | Train MAE: 0.008817 Val MSE: 0.004606 | Val MAE: 0.043258\n",
      "Epoch [73/100] Train Loss: 0.000111 | Train MSE: 0.000111 | Train MAE: 0.007814 Val MSE: 0.004327 | Val MAE: 0.041912\n",
      "Epoch [74/100] Train Loss: 0.000135 | Train MSE: 0.000135 | Train MAE: 0.008445 Val MSE: 0.004240 | Val MAE: 0.041419\n",
      "Epoch [75/100] Train Loss: 0.000118 | Train MSE: 0.000118 | Train MAE: 0.007958 Val MSE: 0.004317 | Val MAE: 0.042180\n",
      "Epoch [76/100] Train Loss: 0.000150 | Train MSE: 0.000150 | Train MAE: 0.008780 Val MSE: 0.005432 | Val MAE: 0.047764\n",
      "Epoch [77/100] Train Loss: 0.000126 | Train MSE: 0.000126 | Train MAE: 0.008128 Val MSE: 0.004273 | Val MAE: 0.041764\n",
      "Epoch [78/100] Train Loss: 0.000116 | Train MSE: 0.000116 | Train MAE: 0.007882 Val MSE: 0.004082 | Val MAE: 0.040580\n",
      "Epoch [79/100] Train Loss: 0.000106 | Train MSE: 0.000106 | Train MAE: 0.007616 Val MSE: 0.004061 | Val MAE: 0.040885\n",
      "Epoch [80/100] Train Loss: 0.000115 | Train MSE: 0.000115 | Train MAE: 0.007886 Val MSE: 0.004039 | Val MAE: 0.040669\n",
      "Epoch [81/100] Train Loss: 0.000100 | Train MSE: 0.000100 | Train MAE: 0.007412 Val MSE: 0.004301 | Val MAE: 0.041444\n",
      "Epoch [82/100] Train Loss: 0.000108 | Train MSE: 0.000108 | Train MAE: 0.007619 Val MSE: 0.004051 | Val MAE: 0.041395\n",
      "Epoch [83/100] Train Loss: 0.000107 | Train MSE: 0.000107 | Train MAE: 0.007603 Val MSE: 0.004050 | Val MAE: 0.040897\n",
      "Epoch [84/100] Train Loss: 0.000115 | Train MSE: 0.000115 | Train MAE: 0.007792 Val MSE: 0.004576 | Val MAE: 0.044262\n",
      "Epoch [85/100] Train Loss: 0.000151 | Train MSE: 0.000151 | Train MAE: 0.008742 Val MSE: 0.004290 | Val MAE: 0.041144\n",
      "Epoch [86/100] Train Loss: 0.000108 | Train MSE: 0.000108 | Train MAE: 0.007582 Val MSE: 0.004479 | Val MAE: 0.043176\n",
      "Epoch [87/100] Train Loss: 0.000093 | Train MSE: 0.000093 | Train MAE: 0.007150 Val MSE: 0.004360 | Val MAE: 0.042127\n",
      "Epoch [88/100] Train Loss: 0.000100 | Train MSE: 0.000100 | Train MAE: 0.007366 Val MSE: 0.004655 | Val MAE: 0.043708\n",
      "Epoch [89/100] Train Loss: 0.000210 | Train MSE: 0.000210 | Train MAE: 0.009436 Val MSE: 0.004088 | Val MAE: 0.041129\n",
      "Epoch [90/100] Train Loss: 0.000096 | Train MSE: 0.000096 | Train MAE: 0.007210 Val MSE: 0.004546 | Val MAE: 0.043092\n",
      "Epoch [91/100] Train Loss: 0.000100 | Train MSE: 0.000100 | Train MAE: 0.007292 Val MSE: 0.004230 | Val MAE: 0.041641\n",
      "Epoch [92/100] Train Loss: 0.000091 | Train MSE: 0.000091 | Train MAE: 0.007029 Val MSE: 0.004148 | Val MAE: 0.040981\n",
      "Epoch [93/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.006978 Val MSE: 0.004238 | Val MAE: 0.041585\n",
      "Epoch [94/100] Train Loss: 0.000085 | Train MSE: 0.000085 | Train MAE: 0.006850 Val MSE: 0.004142 | Val MAE: 0.041302\n",
      "Epoch [95/100] Train Loss: 0.000100 | Train MSE: 0.000100 | Train MAE: 0.007312 Val MSE: 0.004219 | Val MAE: 0.041423\n",
      "Epoch [96/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.006994 Val MSE: 0.004375 | Val MAE: 0.042199\n",
      "Epoch [97/100] Train Loss: 0.000093 | Train MSE: 0.000093 | Train MAE: 0.007073 Val MSE: 0.003769 | Val MAE: 0.039454\n",
      "Epoch [98/100] Train Loss: 0.000100 | Train MSE: 0.000100 | Train MAE: 0.007305 Val MSE: 0.004187 | Val MAE: 0.041282\n",
      "Epoch [99/100] Train Loss: 0.000111 | Train MSE: 0.000111 | Train MAE: 0.007581 Val MSE: 0.004336 | Val MAE: 0.041957\n",
      "Epoch [100/100] Train Loss: 0.000081 | Train MSE: 0.000081 | Train MAE: 0.006706 Val MSE: 0.004190 | Val MAE: 0.041327\n"
     ]
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "        d_model=128, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        num_decoder_layers=4, \n",
    "        dim_feedforward=256, \n",
    "        dropout=0.2).to(device)\n",
    "\n",
    "#损失函数MSE和优化器Adam\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_mse, train_mae = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_mse, val_mae = validate_one_epoch(model, test_loader, criterion, device)\n",
    "    \n",
    "    #保存在验证集上表现最好的模型\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "        torch.save(model.state_dict(), \"trans_best_model4.pth\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.6f} | Train MSE: {train_mse:.6f} | Train MAE: {train_mae:.6f} \"\n",
    "          f\"Val MSE: {val_mse:.6f} | Val MAE: {val_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c5b08f9-fc4d-4657-9826-97726d1008ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.058256 | Train MSE: 0.058256 | Train MAE: 0.143336 Val MSE: 0.021283 | Val MAE: 0.117271\n",
      "Epoch [2/100] Train Loss: 0.004535 | Train MSE: 0.004535 | Train MAE: 0.049831 Val MSE: 0.018438 | Val MAE: 0.099311\n",
      "Epoch [3/100] Train Loss: 0.003046 | Train MSE: 0.003046 | Train MAE: 0.040301 Val MSE: 0.014156 | Val MAE: 0.086030\n",
      "Epoch [4/100] Train Loss: 0.002409 | Train MSE: 0.002409 | Train MAE: 0.035586 Val MSE: 0.013019 | Val MAE: 0.081905\n",
      "Epoch [5/100] Train Loss: 0.002045 | Train MSE: 0.002045 | Train MAE: 0.032680 Val MSE: 0.011264 | Val MAE: 0.075367\n",
      "Epoch [6/100] Train Loss: 0.001816 | Train MSE: 0.001816 | Train MAE: 0.030750 Val MSE: 0.009867 | Val MAE: 0.070816\n",
      "Epoch [7/100] Train Loss: 0.001632 | Train MSE: 0.001632 | Train MAE: 0.029160 Val MSE: 0.010185 | Val MAE: 0.064934\n",
      "Epoch [8/100] Train Loss: 0.001525 | Train MSE: 0.001525 | Train MAE: 0.028097 Val MSE: 0.008258 | Val MAE: 0.063978\n",
      "Epoch [9/100] Train Loss: 0.001344 | Train MSE: 0.001344 | Train MAE: 0.026479 Val MSE: 0.007911 | Val MAE: 0.059769\n",
      "Epoch [10/100] Train Loss: 0.001239 | Train MSE: 0.001239 | Train MAE: 0.025473 Val MSE: 0.007685 | Val MAE: 0.058721\n",
      "Epoch [11/100] Train Loss: 0.001137 | Train MSE: 0.001137 | Train MAE: 0.024418 Val MSE: 0.008925 | Val MAE: 0.067846\n",
      "Epoch [12/100] Train Loss: 0.000991 | Train MSE: 0.000991 | Train MAE: 0.022730 Val MSE: 0.006295 | Val MAE: 0.052289\n",
      "Epoch [13/100] Train Loss: 0.000909 | Train MSE: 0.000909 | Train MAE: 0.021706 Val MSE: 0.006281 | Val MAE: 0.052001\n",
      "Epoch [14/100] Train Loss: 0.000891 | Train MSE: 0.000891 | Train MAE: 0.021199 Val MSE: 0.005572 | Val MAE: 0.047889\n",
      "Epoch [15/100] Train Loss: 0.000737 | Train MSE: 0.000737 | Train MAE: 0.019601 Val MSE: 0.006948 | Val MAE: 0.057571\n",
      "Epoch [16/100] Train Loss: 0.000701 | Train MSE: 0.000701 | Train MAE: 0.019123 Val MSE: 0.006298 | Val MAE: 0.051061\n",
      "Epoch [17/100] Train Loss: 0.000644 | Train MSE: 0.000644 | Train MAE: 0.018348 Val MSE: 0.005780 | Val MAE: 0.049913\n",
      "Epoch [18/100] Train Loss: 0.000605 | Train MSE: 0.000605 | Train MAE: 0.017761 Val MSE: 0.005695 | Val MAE: 0.048136\n",
      "Epoch [19/100] Train Loss: 0.000581 | Train MSE: 0.000581 | Train MAE: 0.017453 Val MSE: 0.005806 | Val MAE: 0.049861\n",
      "Epoch [20/100] Train Loss: 0.000526 | Train MSE: 0.000526 | Train MAE: 0.016647 Val MSE: 0.005976 | Val MAE: 0.050232\n",
      "Epoch [21/100] Train Loss: 0.000554 | Train MSE: 0.000554 | Train MAE: 0.016937 Val MSE: 0.005435 | Val MAE: 0.048229\n",
      "Epoch [22/100] Train Loss: 0.000501 | Train MSE: 0.000501 | Train MAE: 0.016231 Val MSE: 0.005591 | Val MAE: 0.047685\n",
      "Epoch [23/100] Train Loss: 0.000479 | Train MSE: 0.000479 | Train MAE: 0.015813 Val MSE: 0.006114 | Val MAE: 0.050295\n",
      "Epoch [24/100] Train Loss: 0.000429 | Train MSE: 0.000429 | Train MAE: 0.015109 Val MSE: 0.005379 | Val MAE: 0.047760\n",
      "Epoch [25/100] Train Loss: 0.000447 | Train MSE: 0.000447 | Train MAE: 0.015277 Val MSE: 0.005999 | Val MAE: 0.050924\n",
      "Epoch [26/100] Train Loss: 0.000446 | Train MSE: 0.000446 | Train MAE: 0.015134 Val MSE: 0.005977 | Val MAE: 0.049261\n",
      "Epoch [27/100] Train Loss: 0.000362 | Train MSE: 0.000362 | Train MAE: 0.013887 Val MSE: 0.005616 | Val MAE: 0.047245\n",
      "Epoch [28/100] Train Loss: 0.000360 | Train MSE: 0.000360 | Train MAE: 0.013752 Val MSE: 0.005659 | Val MAE: 0.048186\n",
      "Epoch [29/100] Train Loss: 0.000325 | Train MSE: 0.000325 | Train MAE: 0.013143 Val MSE: 0.005016 | Val MAE: 0.045656\n",
      "Epoch [30/100] Train Loss: 0.000306 | Train MSE: 0.000306 | Train MAE: 0.012778 Val MSE: 0.004782 | Val MAE: 0.045603\n",
      "Epoch [31/100] Train Loss: 0.000294 | Train MSE: 0.000294 | Train MAE: 0.012492 Val MSE: 0.004802 | Val MAE: 0.045245\n",
      "Epoch [32/100] Train Loss: 0.000413 | Train MSE: 0.000413 | Train MAE: 0.013904 Val MSE: 0.005407 | Val MAE: 0.046863\n",
      "Epoch [33/100] Train Loss: 0.000285 | Train MSE: 0.000285 | Train MAE: 0.012318 Val MSE: 0.005459 | Val MAE: 0.046785\n",
      "Epoch [34/100] Train Loss: 0.000240 | Train MSE: 0.000240 | Train MAE: 0.011401 Val MSE: 0.005656 | Val MAE: 0.047612\n",
      "Epoch [35/100] Train Loss: 0.000240 | Train MSE: 0.000240 | Train MAE: 0.011353 Val MSE: 0.006430 | Val MAE: 0.053859\n",
      "Epoch [36/100] Train Loss: 0.000236 | Train MSE: 0.000236 | Train MAE: 0.011291 Val MSE: 0.005442 | Val MAE: 0.046746\n",
      "Epoch [37/100] Train Loss: 0.000241 | Train MSE: 0.000241 | Train MAE: 0.011347 Val MSE: 0.005451 | Val MAE: 0.047889\n",
      "Epoch [38/100] Train Loss: 0.000249 | Train MSE: 0.000249 | Train MAE: 0.011334 Val MSE: 0.005095 | Val MAE: 0.044951\n",
      "Epoch [39/100] Train Loss: 0.000233 | Train MSE: 0.000233 | Train MAE: 0.011119 Val MSE: 0.005247 | Val MAE: 0.046394\n",
      "Epoch [40/100] Train Loss: 0.000184 | Train MSE: 0.000184 | Train MAE: 0.010045 Val MSE: 0.005275 | Val MAE: 0.046591\n",
      "Epoch [41/100] Train Loss: 0.000221 | Train MSE: 0.000221 | Train MAE: 0.010746 Val MSE: 0.005207 | Val MAE: 0.045520\n",
      "Epoch [42/100] Train Loss: 0.000185 | Train MSE: 0.000185 | Train MAE: 0.010033 Val MSE: 0.006745 | Val MAE: 0.050362\n",
      "Epoch [43/100] Train Loss: 0.000189 | Train MSE: 0.000189 | Train MAE: 0.010127 Val MSE: 0.005191 | Val MAE: 0.046207\n",
      "Epoch [44/100] Train Loss: 0.000301 | Train MSE: 0.000301 | Train MAE: 0.011877 Val MSE: 0.005065 | Val MAE: 0.045755\n",
      "Epoch [45/100] Train Loss: 0.000170 | Train MSE: 0.000170 | Train MAE: 0.009617 Val MSE: 0.004878 | Val MAE: 0.045399\n",
      "Epoch [46/100] Train Loss: 0.000165 | Train MSE: 0.000165 | Train MAE: 0.009413 Val MSE: 0.005049 | Val MAE: 0.045828\n",
      "Epoch [47/100] Train Loss: 0.000153 | Train MSE: 0.000153 | Train MAE: 0.009146 Val MSE: 0.005423 | Val MAE: 0.046782\n",
      "Epoch [48/100] Train Loss: 0.000178 | Train MSE: 0.000178 | Train MAE: 0.009755 Val MSE: 0.005216 | Val MAE: 0.046658\n",
      "Epoch [49/100] Train Loss: 0.000212 | Train MSE: 0.000212 | Train MAE: 0.010395 Val MSE: 0.005152 | Val MAE: 0.046266\n",
      "Epoch [50/100] Train Loss: 0.000140 | Train MSE: 0.000140 | Train MAE: 0.008769 Val MSE: 0.005193 | Val MAE: 0.047557\n",
      "Epoch [51/100] Train Loss: 0.000145 | Train MSE: 0.000145 | Train MAE: 0.008836 Val MSE: 0.005260 | Val MAE: 0.046990\n",
      "Epoch [52/100] Train Loss: 0.000148 | Train MSE: 0.000148 | Train MAE: 0.008931 Val MSE: 0.005351 | Val MAE: 0.047691\n",
      "Epoch [53/100] Train Loss: 0.000172 | Train MSE: 0.000172 | Train MAE: 0.009404 Val MSE: 0.005419 | Val MAE: 0.048450\n",
      "Epoch [54/100] Train Loss: 0.000123 | Train MSE: 0.000123 | Train MAE: 0.008243 Val MSE: 0.005731 | Val MAE: 0.049208\n",
      "Epoch [55/100] Train Loss: 0.000141 | Train MSE: 0.000141 | Train MAE: 0.008590 Val MSE: 0.005340 | Val MAE: 0.047870\n",
      "Epoch [56/100] Train Loss: 0.000175 | Train MSE: 0.000175 | Train MAE: 0.009524 Val MSE: 0.005267 | Val MAE: 0.048316\n",
      "Epoch [57/100] Train Loss: 0.000119 | Train MSE: 0.000119 | Train MAE: 0.008054 Val MSE: 0.005562 | Val MAE: 0.048280\n",
      "Epoch [58/100] Train Loss: 0.000128 | Train MSE: 0.000128 | Train MAE: 0.008321 Val MSE: 0.005822 | Val MAE: 0.049105\n",
      "Epoch [59/100] Train Loss: 0.000112 | Train MSE: 0.000112 | Train MAE: 0.007850 Val MSE: 0.005453 | Val MAE: 0.048267\n",
      "Epoch [60/100] Train Loss: 0.000354 | Train MSE: 0.000354 | Train MAE: 0.012160 Val MSE: 0.005204 | Val MAE: 0.046488\n",
      "Epoch [61/100] Train Loss: 0.000128 | Train MSE: 0.000128 | Train MAE: 0.008349 Val MSE: 0.005286 | Val MAE: 0.046067\n",
      "Epoch [62/100] Train Loss: 0.000109 | Train MSE: 0.000109 | Train MAE: 0.007727 Val MSE: 0.005439 | Val MAE: 0.047571\n",
      "Epoch [63/100] Train Loss: 0.000103 | Train MSE: 0.000103 | Train MAE: 0.007524 Val MSE: 0.005245 | Val MAE: 0.046682\n",
      "Epoch [64/100] Train Loss: 0.000100 | Train MSE: 0.000100 | Train MAE: 0.007457 Val MSE: 0.005504 | Val MAE: 0.047809\n",
      "Epoch [65/100] Train Loss: 0.000119 | Train MSE: 0.000119 | Train MAE: 0.007920 Val MSE: 0.005627 | Val MAE: 0.048133\n",
      "Epoch [66/100] Train Loss: 0.000100 | Train MSE: 0.000100 | Train MAE: 0.007413 Val MSE: 0.005306 | Val MAE: 0.046567\n",
      "Epoch [67/100] Train Loss: 0.000151 | Train MSE: 0.000151 | Train MAE: 0.008650 Val MSE: 0.005752 | Val MAE: 0.050388\n",
      "Epoch [68/100] Train Loss: 0.000151 | Train MSE: 0.000151 | Train MAE: 0.008694 Val MSE: 0.005870 | Val MAE: 0.049418\n",
      "Epoch [69/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.007038 Val MSE: 0.005623 | Val MAE: 0.048370\n",
      "Epoch [70/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.007021 Val MSE: 0.005925 | Val MAE: 0.049685\n",
      "Epoch [71/100] Train Loss: 0.000095 | Train MSE: 0.000095 | Train MAE: 0.007266 Val MSE: 0.005613 | Val MAE: 0.046960\n",
      "Epoch [72/100] Train Loss: 0.000280 | Train MSE: 0.000280 | Train MAE: 0.010895 Val MSE: 0.005759 | Val MAE: 0.049070\n",
      "Epoch [73/100] Train Loss: 0.000099 | Train MSE: 0.000099 | Train MAE: 0.007365 Val MSE: 0.005674 | Val MAE: 0.047816\n",
      "Epoch [74/100] Train Loss: 0.000228 | Train MSE: 0.000228 | Train MAE: 0.009194 Val MSE: 0.005065 | Val MAE: 0.045929\n",
      "Epoch [75/100] Train Loss: 0.000123 | Train MSE: 0.000123 | Train MAE: 0.008114 Val MSE: 0.005358 | Val MAE: 0.046863\n",
      "Epoch [76/100] Train Loss: 0.000089 | Train MSE: 0.000089 | Train MAE: 0.007013 Val MSE: 0.005686 | Val MAE: 0.047904\n",
      "Epoch [77/100] Train Loss: 0.000092 | Train MSE: 0.000092 | Train MAE: 0.007038 Val MSE: 0.005407 | Val MAE: 0.046043\n",
      "Epoch [78/100] Train Loss: 0.000087 | Train MSE: 0.000087 | Train MAE: 0.006943 Val MSE: 0.005760 | Val MAE: 0.047584\n",
      "Epoch [79/100] Train Loss: 0.000083 | Train MSE: 0.000083 | Train MAE: 0.006805 Val MSE: 0.005343 | Val MAE: 0.046671\n",
      "Epoch [80/100] Train Loss: 0.000084 | Train MSE: 0.000084 | Train MAE: 0.006810 Val MSE: 0.005750 | Val MAE: 0.047800\n",
      "Epoch [81/100] Train Loss: 0.000140 | Train MSE: 0.000140 | Train MAE: 0.008323 Val MSE: 0.005788 | Val MAE: 0.047833\n",
      "Epoch [82/100] Train Loss: 0.000088 | Train MSE: 0.000088 | Train MAE: 0.006926 Val MSE: 0.005622 | Val MAE: 0.047790\n",
      "Epoch [83/100] Train Loss: 0.000097 | Train MSE: 0.000097 | Train MAE: 0.007095 Val MSE: 0.006250 | Val MAE: 0.049109\n",
      "Epoch [84/100] Train Loss: 0.000087 | Train MSE: 0.000087 | Train MAE: 0.006885 Val MSE: 0.005668 | Val MAE: 0.047537\n",
      "Epoch [85/100] Train Loss: 0.000079 | Train MSE: 0.000079 | Train MAE: 0.006605 Val MSE: 0.005818 | Val MAE: 0.048895\n",
      "Epoch [86/100] Train Loss: 0.000088 | Train MSE: 0.000088 | Train MAE: 0.006874 Val MSE: 0.006092 | Val MAE: 0.048921\n",
      "Epoch [87/100] Train Loss: 0.000091 | Train MSE: 0.000091 | Train MAE: 0.006980 Val MSE: 0.005295 | Val MAE: 0.046649\n",
      "Epoch [88/100] Train Loss: 0.000099 | Train MSE: 0.000099 | Train MAE: 0.007091 Val MSE: 0.005701 | Val MAE: 0.048502\n",
      "Epoch [89/100] Train Loss: 0.000145 | Train MSE: 0.000145 | Train MAE: 0.008356 Val MSE: 0.005785 | Val MAE: 0.048722\n",
      "Epoch [90/100] Train Loss: 0.000088 | Train MSE: 0.000088 | Train MAE: 0.006807 Val MSE: 0.005766 | Val MAE: 0.048530\n",
      "Epoch [91/100] Train Loss: 0.000071 | Train MSE: 0.000071 | Train MAE: 0.006289 Val MSE: 0.006040 | Val MAE: 0.049565\n",
      "Epoch [92/100] Train Loss: 0.000069 | Train MSE: 0.000069 | Train MAE: 0.006209 Val MSE: 0.005805 | Val MAE: 0.048674\n",
      "Epoch [93/100] Train Loss: 0.000079 | Train MSE: 0.000079 | Train MAE: 0.006577 Val MSE: 0.005955 | Val MAE: 0.048694\n",
      "Epoch [94/100] Train Loss: 0.000187 | Train MSE: 0.000187 | Train MAE: 0.008471 Val MSE: 0.004718 | Val MAE: 0.044408\n",
      "Epoch [95/100] Train Loss: 0.000118 | Train MSE: 0.000118 | Train MAE: 0.007750 Val MSE: 0.005230 | Val MAE: 0.046095\n",
      "Epoch [96/100] Train Loss: 0.000068 | Train MSE: 0.000068 | Train MAE: 0.006137 Val MSE: 0.005460 | Val MAE: 0.046649\n",
      "Epoch [97/100] Train Loss: 0.000065 | Train MSE: 0.000065 | Train MAE: 0.005985 Val MSE: 0.005270 | Val MAE: 0.046311\n",
      "Epoch [98/100] Train Loss: 0.000067 | Train MSE: 0.000067 | Train MAE: 0.006109 Val MSE: 0.005576 | Val MAE: 0.047232\n",
      "Epoch [99/100] Train Loss: 0.000065 | Train MSE: 0.000065 | Train MAE: 0.006026 Val MSE: 0.005841 | Val MAE: 0.048092\n",
      "Epoch [100/100] Train Loss: 0.000067 | Train MSE: 0.000067 | Train MAE: 0.006074 Val MSE: 0.005201 | Val MAE: 0.046453\n"
     ]
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "        d_model=128, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        num_decoder_layers=4, \n",
    "        dim_feedforward=256, \n",
    "        dropout=0.2).to(device)\n",
    "\n",
    "# 损失函数(MSE)和优化器(Adam)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_mse, train_mae = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_mse, val_mae = validate_one_epoch(model, test_loader, criterion, device)\n",
    "    \n",
    "    # 保存在验证集上表现最好的模型\n",
    "    if val_mse < best_val_mse:\n",
    "        best_val_mse = val_mse\n",
    "        torch.save(model.state_dict(), \"trans_best_model5.pth\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.6f} | Train MSE: {train_mse:.6f} | Train MAE: {train_mae:.6f} \"\n",
    "          f\"Val MSE: {val_mse:.6f} | Val MAE: {val_mae:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb10ab-d2f8-455f-8d15-648d362bc885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a895aca-62a5-47a1-9c76-b96ac9545e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1629d8c-4b06-461c-846d-2d192deef07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_real(model, dataloader, criterion, device, min_val, max_val):\n",
    "    \n",
    "    model.eval()\n",
    "    running_mse = 0.0\n",
    "    running_mae = 0.0\n",
    "    all_outputs = []  #保存所有批次的预测值\n",
    "    all_labels = []  #保存所有批次的真实值\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y in dataloader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y = y.to(device).squeeze(-1)\n",
    "\n",
    "            outputs = model(x_enc, x_dec).squeeze(-1)\n",
    "\n",
    "            #反归一化预测值和标签\n",
    "            outputs_real = outputs * (max_val - min_val) + min_val\n",
    "            batch_y_real = y * (max_val - min_val) + min_val\n",
    "\n",
    "            #保存预测值和真实值\n",
    "            all_outputs.append(outputs_real.cpu().numpy())\n",
    "            all_labels.append(batch_y_real.cpu().numpy())\n",
    "\n",
    "            #计算反归一化后的MSE和MAE\n",
    "            mse_loss = criterion(outputs_real, batch_y_real)\n",
    "            mae_loss = torch.mean(torch.abs(outputs_real - batch_y_real))\n",
    "\n",
    "            #累积误差\n",
    "            running_mse += mse_loss.item() * x_enc.size(0)\n",
    "            running_mae += mae_loss.item() * x_enc.size(0)\n",
    "\n",
    "    #将所有批次结果拼接为一个数组\n",
    "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    #计算最终的平均误差\n",
    "    epoch_mse = running_mse / len(dataloader.dataset)\n",
    "    epoch_mae = running_mae / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_mse, epoch_mae, all_outputs, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c34029b6-fe57-4e72-b44d-895bfd12667b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7044421-dece-42ef-a1df-505fc4761ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MSE: 3541.315780, MAE: 38.142188\n"
     ]
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "        d_model=128, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        num_decoder_layers=4, \n",
    "        dim_feedforward=512, \n",
    "        dropout=0.2).to(device)\n",
    "#加载最优模型\n",
    "model.load_state_dict(torch.load(\"trans_best_model1.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "min_val = test_data['cnt'].min()\n",
    "max_val = test_data['cnt'].max()\n",
    "\n",
    "mse_test, mae_test, all_outputs1, all_labels1 = validate_real(model, test_loader, criterion, device, min_val, max_val)\n",
    "print(f\"Final Test MSE: {mse_test:.6f}, MAE: {mae_test:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3edac648-ea32-4ca1-8d89-e1d83ab7d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MSE: 3880.610068, MAE: 38.100379\n"
     ]
    }
   ],
   "source": [
    "model = TransformerTimeSeriesModel(\n",
    "        d_model=128, \n",
    "        nhead=8, \n",
    "        num_encoder_layers=4, \n",
    "        num_decoder_layers=4, \n",
    "        dim_feedforward=256, \n",
    "        dropout=0.2).to(device)\n",
    "\n",
    "#加载最优模型\n",
    "model.load_state_dict(torch.load(\"trans_best_model2.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "min_val = test_data['cnt'].min()\n",
    "max_val = test_data['cnt'].max()\n",
    "\n",
    "mse_test, mae_test, all_outputs2, all_labels2 = validate_real(model, test_loader, criterion, device, min_val, max_val)\n",
    "print(f\"Final Test MSE: {mse_test:.6f}, MAE: {mae_test:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "167c4554-4b80-478b-a4cc-11603a352ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MSE: 3921.362306, MAE: 38.948957\n"
     ]
    }
   ],
   "source": [
    "#加载最优模型\n",
    "model.load_state_dict(torch.load(\"trans_best_model3.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "min_val = test_data['cnt'].min()\n",
    "max_val = test_data['cnt'].max()\n",
    "\n",
    "mse_test, mae_test, all_outputs3, all_labels3 = validate_real(model, test_loader, criterion, device, min_val, max_val)\n",
    "print(f\"Final Test MSE: {mse_test:.6f}, MAE: {mae_test:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "685abd48-fb6d-46fc-b687-993cc5e93578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MSE: 3398.251666, MAE: 36.052841\n"
     ]
    }
   ],
   "source": [
    "#加载最优模型\n",
    "model.load_state_dict(torch.load(\"trans_best_model4.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "min_val = test_data['cnt'].min()\n",
    "max_val = test_data['cnt'].max()\n",
    "\n",
    "mse_test, mae_test, all_outputs4, all_labels4 = validate_real(model, test_loader, criterion, device, min_val, max_val)\n",
    "print(f\"Final Test MSE: {mse_test:.6f}, MAE: {mae_test:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "84f3b163-ac30-49b4-a9d6-89a28a729b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MSE: 4366.065759, MAE: 42.720547\n"
     ]
    }
   ],
   "source": [
    "#加载最优模型\n",
    "model.load_state_dict(torch.load(\"trans_best_model5.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "min_val = test_data['cnt'].min()\n",
    "max_val = test_data['cnt'].max()\n",
    "\n",
    "mse_test, mae_test, all_outputs5, all_labels5 = validate_real(model, test_loader, criterion, device, min_val, max_val)\n",
    "print(f\"Final Test MSE: {mse_test:.6f}, MAE: {mae_test:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fd4cc-ed56-488e-be2b-e0beb8aaf84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_mean = (all_labels1 + all_labels2 + all_labels3 + all_labels4 + all_labels55) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c05b6-61cc-400b-88c0-344990a38f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b86a2d-51b1-496e-86d0-ee4f71cb6f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71a4c4-b8a9-4337-ad73-8d69257945db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d145a8ec-5e7c-4729-856b-89fe2810f81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[475.90363 , 292.5068  , 183.29546 , ...,  25.188158, 114.91018 ,\n",
       "        416.84113 ],\n",
       "       [289.34116 , 183.08315 , 217.46425 , ..., 117.05428 , 419.95633 ,\n",
       "        656.3325  ],\n",
       "       [182.75978 , 217.23105 , 240.85158 , ..., 426.6014  , 668.75183 ,\n",
       "        349.09464 ],\n",
       "       ...,\n",
       "       [103.14982 ,  74.03653 ,  43.85506 , ..., 351.79932 , 227.39796 ,\n",
       "        152.7124  ],\n",
       "       [ 74.88401 ,  43.937336,  22.335388, ..., 227.76772 , 153.2399  ,\n",
       "         99.90753 ],\n",
       "       [ 44.3646  ,  22.345345,  10.914853, ..., 153.87631 , 100.1126  ,\n",
       "         69.755005]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outputs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8630816-01e3-4319-a396-414d6081866a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[784., 340., 179., ...,  41., 137., 428.],\n",
       "       [340., 179., 272., ..., 137., 428., 785.],\n",
       "       [179., 272., 323., ..., 428., 785., 384.],\n",
       "       ...,\n",
       "       [ 63.,  44.,  26., ..., 122., 119.,  89.],\n",
       "       [ 44.,  26.,  25., ..., 119.,  89.,  90.],\n",
       "       [ 26.,  25.,   9., ...,  89.,  90.,  61.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b892a-ab65-4d38-ad4d-74d349dc036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs_int = all_outputs.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ef49e-31ff-47cd-abf1-aebcb94ae326",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs_int[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c5e72-b34c-4e50-9ea2-f9c074de4bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_int = all_labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515418cb-d3c2-424e-b49f-186a2266c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels_int[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459980b-976f-411c-9210-c7795436a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b2d0f-b37b-4502-a7ab-4f35611add61",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990be7ae-be37-46ee-85ed-117381821b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['cnt'] = test_data['cnt']* (max_val - min_val) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009a4c5-b306-49f4-ba30-b18365dc06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f312f-779b-49d7-9b97-add35b59c26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
